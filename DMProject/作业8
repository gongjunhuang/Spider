a.EM算法与K-means算法的联系

K-means算法是一种无监督的学习方法。假设给定训练样本是{𝑥(1), … , 𝑥(𝑚)}，每个𝑥
(𝑖) ∈ 𝑅𝑛,但没有了 y，也就是说我们不知道每个𝑥(𝑖)对应的类别。K-means 算法就是用来将这些样本聚类成 k 个簇。同 EM 算法一样，首先我们先定义 k-means 的目标函数，如下：
    J(c, μ) = ∑‖𝑚𝑖=1𝑥(𝑖) − 𝜇𝑐(𝑖)‖2

J 函数表示每个样本点到其质心的距离平方和。 K-means 是要将 J 调整到最小。假
设当前 J 没有达到最小值，那么首先可以固定每个类的质心𝜇𝑗，调整每个样例的所属的类
别𝑐(𝑖)来让 J 函数减少，同样，固定𝑐(𝑖)，调整每个类的质心𝜇𝑗也可以使 J 减小。 这两个过程就是内循环中使 J 单调递减的过程。

对应于 K-means 来说就是EM一开始不知道每个样例𝑥(𝑖)对应隐含变量，也就是最佳类别𝑐
(𝑖)。最开始可以随便指定一个𝑐(𝑖)给它，然后为了让目标函数 J 最小，我们求出在给定 c 情况下， J 最小时的𝜇𝑗（每个类的质心坐标），然而此时发现，可以有更好的𝑐(𝑖)（质心与样例𝑥(𝑖)距离最小的类别）指定给样例𝑥(𝑖)，那么𝑐(𝑖)得到重新调整，上述过程就开始重复了，直到没有更好的𝑐(𝑖)指定。 这样从K-means 里我们可以看出它其实就是 EM 的体现。


b.EM算法要解决的问题

EM算法解决这个的思路是使用启发式的迭代方法，既然我们无法直接求出模型分布参数，那么我们可以先猜想隐含数据（EM算法的E步），接着基于观察数据和猜测的隐含数据一起来极大化对数似然，求解我们的模型参数（EM算法的M步)。由于我们之前的隐藏数据是猜测的，所以此时得到的模型参数一般还不是我们想要的结果。不过没关系，我们基于当前得到的模型参数，继续猜测隐含数据（EM算法的E步），然后继续极大化对数似然，求解我们的模型参数（EM算法的M步)。以此类推，不断的迭代下去，直到模型分布参数基本无变化，算法收敛，找到合适的模型参数。

　　　　从上面的描述可以看出，EM算法是迭代求解最大值的算法，同时算法在每一次迭代时分为两步，E步和M步。一轮轮迭代更新隐含数据和模型分布参数，直到收敛，即得到我们需要的模型参数。

c.EM算法的推导
d.EM算法的收敛性思考
e.最大似然估计与EM算法的关系

极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。

求最大似然函数估计值的一般步骤：

（1）写出似然函数；

（2）对似然函数取对数，并整理；

（3）求导数，令导数为0，得到似然方程；

（4）解似然方程，得到的参数即为所求；

EM算法是在概率（probabilistic）模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量（Latent Variable）。EM经常用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。

EM算法经过两个步骤交替进行计算：

第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。

M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。

可以说EM就是为解决数据缺失问题而生的，EM算法的主要目的是提供一个简单的迭代算法计算后验密度函数，它的最大优点是简单和稳定，但也容易陷入局部最优。
