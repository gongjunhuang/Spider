第一讲： The learning problem

1. What is Machine Learning
什么是“学习”？学习就是人类通过观察、积累经验，掌握某项技能或能力。就好像我们从小学习识别字母、认识汉字，就是学习的过程。而机器学习（Machine Learning），顾名思义，就是让机器（计算机）也能向人类一样，通过观察大量的数据和训练，发现事物规律，获得某种分析问题、解决问题的能力。

learning: acquiring skill with experience accumulated from observations 

machine learning: acquiring skill with experience accumulated/computed from data

机器学习可以被定义为：Improving some performance measure with experence computed from data. 也就是机器从数据中总结经验，从数据中找出某种规律或者模型，并用它来解决实际问题。


什么情况下会使用机器学习来解决问题呢？其实，目前机器学习的应用非常广泛，基本上任何场合都能够看到它的身影。其应用场合大致可归纳为三个条件：

----事物本身存在某种潜在规律

----某些问题难以使用普通编程解决

----有大量的数据样本可供使用


2. Applications of Machine Learning
机器学习在我们的衣、食、住、行、教育、娱乐等各个方面都有着广泛的应用，我们的生活处处都离不开机器学习。比如，打开购物网站，网站就会给我们自动推荐我们可能会喜欢的商品；电影频道会根据用户的浏览记录和观影记录，向不同用户推荐他们可能喜欢的电影等等，到处都有机器学习的影子。

3. Components of Machine Learning
本系列的课程对机器学习问题有一些基本的术语需要注意一下：

输入x

输出y

目标函数f，即最接近实际样本分布的规律

训练样本data

假设hypothesis，一个机器学习模型对应了很多不同的hypothesis，通过演算法A，选择一个最佳的hypothesis对应的函数称为矩g，g能最好地表示事物的内在规律，也是我们最终想要得到的模型表达式。

对于理想的目标函数f，我们是不知道的，我们手上拿到的是一些训练样本D，假设是监督式学习，其中有输入x，也有输出y。机器学习的过程，就是根据先验知识选择模型，该模型对应的hypothesis set（用H表示），H中包含了许多不同的hypothesis，通过演算法A，在训练样本D上进行训练，选择出一个最好的hypothes，对应的函数表达式g就是我们最终要求的。一般情况下，g能最接近目标函数f，这样，机器学习的整个流程就完成了。







第二课：  Learning to Answer Yes/No

1. Perceptron Hypothesis Set

在机器学习的整个流程中，有一个部分非常重要：就是模型选择，即Hypothesis Set。选择什么样的模型，很大程度上会影响机器学习的效果和表现。一个简单常用的Hypothesis Set：感知机（Perceptron）。

在银行是否给用户发信用卡的例子中，我们把用户的个人信息作为特征向量x，令总共有d个特征，每个特征赋予不同的权重w，表示该特征对输出（是否发信用卡）的影响有多大。那所有特征的加权和的值与一个设定的阈值threshold进行比较：大于这个阈值，输出为+1，即发信用卡；小于这个阈值，输出为-1，即不发信用卡。感知机模型，就是当特征加权和与阈值的差大于或等于0，则输出h(x)=1；当特征加权和与阈值的差小于0，则输出h(x)=-1，而我们的目的就是计算出所有权值w和阈值threshold。

为了计算方便，通常我们将阈值threshold当做w0，引入一个x0=1的量与w0相乘，这样就把threshold也转变成了权值w0，简化了计算。

h(x) = sign(WTX)

假设Perceptrons在二维平面上，即h(x)=sign(w0+w1x1+w2x2)。其中，w0+w1x1+w2x2=0是平面上一条分类直线，直线一侧是正类（+1），直线另一侧是负类（-1）。权重w不同，对应于平面上不同的直线。


那么，我们所说的Perceptron，在这个模型上就是一条直线，称之为linear(binary) classifiers。注意一下，感知器线性分类不限定在二维空间中，在3D中，线性分类用平面表示，在更高维度中，线性分类用超平面表示，即只要是形如wTx的线性模型就都属于linear(binary) classifiers。


2. Perceptron Learning Algorithm(PLA)
从上面可知hypothesis set由许多条直线构成。接下来，就是如何设计一个算法A，来选择一个最好的直线，能将平面上所有的正类和负类完全分开，也就是找到最好的g，使g≈f。

如何找到这样一条最好的直线呢？我们可以使用逐点修正的思想，首先在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是PLA思想所在。

首先随机选择一条直线进行分类。然后找到第一个分类错误的点，如果这个点表示正类，被误分为负类，即wTtxn(t)<0，那表示w和x夹角大于90度，其中w是直线的法向量。所以，x被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使w和x夹角小于90度。通常做法是w←w+yx, y=1，如图右上角所示，一次或多次更新后的w+yx与x夹角小于90度，能保证x位于直线的上侧，则对误分为负类的错误点完成了直线修正。

同理，如果是误分为正类的点，即wTtxn(t)>0，那表示w和x夹角小于90度，其中w是直线的法向量。所以，x被误分在直线的上侧，修正的方法就是使w和x夹角大于90度。通常做法是w←w+yx, y=−1，如图右下角所示，一次或多次更新后的w+yx与x夹角大于90度，能保证x位于直线的下侧，则对误分为正类的错误点也完成了直线修正。

按照这种思想，遇到个错误点就进行修正，不断迭代。要注意一点：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）。


3. Guarantee of PLA
PLA什么时候会停下来呢？根据PLA的定义，当找到一条直线，能将所有平面上的点都分类正确，那么PLA就停止了。要达到这个终止条件，就必须保证D是线性可分（linear separable）。如果是非线性可分的，那么，PLA就不会停止。


4. Non-Separable Data
上一部分，我们证明了线性可分的情况下，PLA是可以停下来并正确分类的，但对于非线性可分的情况，wf实际上并不存在，那么之前的推导并不成立，PLA不一定会停下来。所以，PLA虽然实现简单，但也有缺点。对于非线性可分的情况，我们可以把它当成是数据集D中掺杂了一下noise，事实上，大多数情况下我们遇到的D，都或多或少地掺杂了noise。




第三课： Types of Learning

1. Learning with Different Output Space Y
在银行根据用户个人情况判断是否给他发信用卡的例子中，这是一个典型的二元分类（binary classification）问题。也就是说输出只有两个，一般y={-1, +1}，-1代表不发信用卡（负类），+1代表发信用卡（正类）。

二元分类的问题很常见，包括信用卡发放、垃圾邮件判别、患者疾病诊断、答案正确性估计等等。二元分类是机器学习领域非常核心和基本的问题。二元分类有线性模型也有非线性模型，根据实际问题情况，选择不同的模型。

除了二元分类，也有多元分类（Multiclass Classification）问题。顾名思义，多元分类的输出多于两个，y={1, 2, … , K}, K>2. 一般多元分类的应用有数字识别、图片内容识别等等。

二元分类和多元分类都属于分类问题，它们的输出都是离散值。二对于另外一种情况，比如训练模型，预测房屋价格、股票收益多少等，这类问题的输出y=R，即范围在整个实数空间，是连续的。这类问题，我们把它叫做回归（Regression）。最简单的线性回归是一种典型的回归模型。

除了分类和回归问题，在自然语言处理等领域中，还会用到一种机器学习问题：结构化学习（Structured Learning）。结构化学习的输出空间包含了某种结构在里面，它的一些解法通常是从多分类问题延伸而来的，比较复杂。本系列课程不会详细介绍Structured Learning，有兴趣的读者可以自行对它进行更深入的研究。

简单总结一下，机器学习按照输出空间划分的话，包括二元分类、多元分类、回归、结构化学习等不同的类型。其中二元分类和回归是最基础、最核心的两个类型。



2. Learning with Different Data Label yn
如果我们拿到的训练样本D既有输入特征x，也有输出yn，那么我们把这种类型的学习称为监督式学习（Supervised Learning）。监督式学习可以是二元分类、多元分类或者是回归，最重要的是知道输出标签yn。与监督式学习相对立的另一种类型是非监督式学习（Unsupervised learning）。非监督式学习是没有输出标签yn的，典型的非监督式学习包括：聚类（clustering）问题，比如对网页上新闻的自动分类；密度估计，比如交通路况分析；异常检测，比如用户网络流量监测。通常情况下，非监督式学习更复杂一些，而且非监督的问题很多都可以使用监督式学习的一些算法思想来实现。

介于监督式和非监督式学习之间的叫做半监督式学习（Semi-supervised Learning）。顾名思义，半监督式学习就是说一部分数据有输出标签yn，而另一部分数据没有输出标签yn。在实际应用中，半监督式学习有时候是必须的，比如医药公司对某些药物进行检测，考虑到成本和实验人群限制等问题，只有一部分数据有输出标签yn。

监督式、非监督式、半监督式学习是机器学习领域三个主要类型。除此之外，还有一种非常重要的类型：增强学习（Reinforcement Learning）。增强学习中，我们给模型或系统一些输入，但是给不了我们希望的真实的输出y，根据模型的输出反馈，如果反馈结果良好，更接近真实输出，就给其正向激励，如果反馈结果不好，偏离真实输出，就给其反向激励。不断通过“反馈-修正”这种形式，一步一步让模型学习的更好，这就是增强学习的核心所在。增强学习可以类比成训练宠物的过程，比如我们要训练狗狗坐下，但是狗狗无法直接听懂我们的指令“sit down”。在训练过程中，我们给狗狗示意，如果它表现得好，我们就给他奖励，如果它做跟sit down完全无关的动作，我们就给它小小的惩罚。这样不断修正狗狗的动作，最终能让它按照我们的指令来行动。实际生活中，增强学习的例子也很多，比如根据用户点击、选择而不断改进的广告系统

简单总结一下，机器学习按照数据输出标签yn划分的话，包括监督式学习、非监督式学习、半监督式学习和增强学习等。其中，监督式学习应用最为广泛。


3. Learning with Different Protocol f(xn,yn)
按照不同的协议，机器学习可以分为三种类型：

Batch Learning

Online

Active Learning

batch learning是一种常见的类型。batch learning获得的训练数据D是一批的，即一次性拿到整个D，对其进行学习建模，得到我们最终的机器学习模型。batch learning在实际应用中最为广泛。

online是一种在线学习模型，数据是实时更新的，根据数据一个个进来，同步更新我们的算法。比如在线邮件过滤系统，根据一封一封邮件的内容，根据当前算法判断是否为垃圾邮件，再根据用户反馈，及时更新当前算法。这是一个动态的过程。之前我们介绍的PLA和增强学习都可以使用online模型。

active learning是近些年来新出现的一种机器学习类型，即让机器具备主动问问题的能力，例如手写数字识别，机器自己生成一个数字或者对它不确定的手写字主动提问。active learning优势之一是在获取样本label比较困难的时候，可以节约时间和成本，只对一些重要的label提出需求。

简单总结一下，按照不同的协议，机器学习可以分为batch, online, active。这三种学习类型分别可以类比为：填鸭式，老师教学以及主动问问题。


4. Learning with Different Input Space X
上面几部分介绍的机器学习分类都是根据输出来分类的，比如根据输出空间进行分类，根据输出y的标记进行分类，根据取得数据和标记的方法进行分类。这部分，我们将谈谈输入X有哪些类型。

输入X的第一种类型就是concrete features。比如说硬币分类问题中硬币的尺寸、重量等；比如疾病诊断中的病人信息等具体特征。concrete features对机器学习来说最容易理解和使用。

第二种类型是raw features。比如说手写数字识别中每个数字所在图片的mxn维像素值；比如语音信号的频谱等。raw features一般比较抽象，经常需要人或者机器来转换为其对应的concrete features，这个转换的过程就是Feature Transform。

第三种类型是abstract features。比如某购物网站做购买预测时，提供给参赛者的是抽象加密过的资料编号或者ID，这些特征X完全是抽象的，没有实际的物理含义。所以对于机器学习来说是比较困难的，需要对特征进行更多的转换和提取。

简单总结一下，根据输入X类型不同，可以分为concetet, raw, abstract。将一些抽象的特征转换为具体的特征，是机器学习过程中非常重要的一个环节。



第四课： Feasibility of Learning

1. Learning is Impossible

----九宫格例子，不同角度看分类都是对的

想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果。机器学习的这种特性被称为没有免费午餐（No Free Lunch）定理。NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。从这个例子来看，NFL说明了无法保证一个机器学习算法在D以外的数据集上一定能分类或预测正确，除非加上一些假设条件。


2. Probability to the Rescue
从上得出的结论是：在训练集D以外的样本上，机器学习的模型是很难，似乎做不到正确预测或分类的。那是否有一些工具或者方法能够对未知的目标函数f做一些推论，让我们的机器学习模型能够变得有用呢？

如果有一个装有很多（数量很大数不过来）橙色球和绿色球的罐子，我们能不能推断橙色球的比例u？统计学上的做法是，从罐子中随机取出N个球，作为样本，计算这N个球中橙色球的比例v，那么就估计出罐子中橙色球的比例约为v。

从概率的角度来说，样本中的v很有可能接近我们未知的u。下面从数学推导的角度来看v与u是否相近。

已知u是罐子里橙色球的比例，v是N个抽取的样本中橙色球的比例。当N足够大的时候，v接近于u。这就是Hoeffding’s inequality：

		P[|v−u|>ϵ]≤2exp(−2ϵ2N)

Hoeffding不等式说明当N很大的时候，v与u相差不会很大，它们之间的差值被限定在ϵ之内。我们把结论v=u称为probably approximately correct(PAC)。



3. Connection to Learning
机器学习中hypothesis与目标函数相等的可能性，类比于罐子中橙色球的概率问题；罐子里的一颗颗弹珠类比于机器学习样本空间的x；橙色的弹珠类比于h(x)与f不相等；绿色的弹珠类比于h(x)与f相等；从罐子中抽取的N个球类比于机器学习的训练样本D，且这两种抽样的样本与总体样本之间都是独立同分布的。所以呢，如果样本N够大，且是独立同分布的，那么，从样本中h(x)≠f(x)的概率就能推导在抽样样本外的所有样本中h(x)≠f(x)的概率是多少。

映射中最关键的点是讲抽样中橙球的概率理解为样本数据集D上h(x)错误的概率，以此推算出在所有数据上h(x)错误的概率，这也是机器学习能够工作的本质，即我们为啥在采样数据上得到了一个假设，就可以推到全局呢？因为两者的错误率是PAC的，只要我们保证前者小，后者也就小了。

引入两个值Ein(h)和Eout(h)。Ein(h)表示在抽样样本中，h(x)与yn不相等的概率；Eout(h)表示实际所有样本中，h(x)与f(x)不相等的概率是多少。

同样，它的Hoeffding’s inequality可以表示为：

         P[|Ein(h)−Eout(h)|>ϵ]≤2exp(−2ϵ2N)

该不等式表明，Ein(h)=Eout(h)也是PAC的。如果Ein(h)≈Eout(h)，Ein(h)很小，那么就能推断出Eout(h)很小，也就是说在该数据分布P下，h与f非常接近，机器学习的模型比较准确。

一般地，h如果是固定的，N很大的时候，Ein(h)≈Eout(h)，但是并不意味着g≈f。因为h是固定的，不能保证Ein(h)足够小，即使Ein(h)≈Eout(h)，也可能使Eout(h)偏大。所以，一般会通过演算法A，选择最好的h，使Ein(h)足够小，从而保证Eout(h)很小。固定的h，使用新数据进行测试，验证其错误率是多少。


4. Connection to Real Learning

根据许多次抽样的到的不同的数据集D，Hoeffding’s inequality保证了大多数的D都是比较好的情形（即对于某个h，保证Ein≈Eout），但是也有可能出现Bad Data，即Ein和Eout差别很大的数据集D，这是小概率事件。

不同的数据集Dn，对于不同的hypothesis，有可能成为Bad Data。只要Dn在某个hypothesis上是Bad Data，那么Dn就是Bad Data。只有当Dn在所有的hypothesis上都是好的数据，才说明Dn不是Bad Data，可以自由选择演算法A进行建模。




第五课： Training versus Testing

1. Recap and Preview

把机器学习的主要目标分成两个核心的问题：

Ein(g)≈Eout(g)
Ein(g)足够小

2. Effective Number of Line

霍夫丁不等式：

        P[|Ein(g)−Eout(g)|>ϵ]≤2⋅M⋅exp(−2ϵ2N)
其中，M表示hypothesis的个数。每个hypothesis下的BAD events Bm级联的形式满足下列不等式：

        P[B1 or B2 or ⋯BM]≤P[B1]+P[B2]+⋯+P[BM]
当M=∞时，上面不等式右边值将会很大，似乎说明BAD events很大，Ein(g)与Eout(g)也并不接近。但是BAD events Bm级联的形式实际上是扩大了上界，union bound过大。这种做法假设各个hypothesis之间没有交集，这是最坏的情况，可是实际上往往不是如此，很多情况下，都是有交集的，也就是说M实际上没那么大。

经过分析，我们得到平面上线的种类是有限的，1个点最多有2种线，2个点最多有4种线，3个点最多有8种线，4个点最多有14（<24）种线等等。我们发现，有效直线的数量总是满足≤2N，其中，N是点的个数。所以，如果我们可以用effective(N)代替M，霍夫丁不等式可以写成：

        P[|Ein(g)−Eout(g)|>ϵ]≤2⋅effective(N)⋅exp(−2ϵ2N)
已知effective(N)<2N，如果能够保证effective(N)<<2N，即不等式右边接近于零，那么即使M无限大，直线的种类也很有限，机器学习也是可能的。


3. Effective Number of Hypotheses
二分类（dichotomy）

dichotomy就是将空间中的点（例如二维平面）用一条直线分成正类（蓝色o）和负类（红色x）。令H是将平面上的点用直线分开的所有hypothesis h的集合，dichotomy H与hypotheses H的关系是：hypotheses H是平面上所有直线的集合，个数可能是无限个，而dichotomy H是平面上能将点完全用直线分开的直线种类，它的上界是2N。接下来，我们要做的就是尝试用dichotomy代替M。

成长函数（growth function），记为mH(H)。成长函数的定义是：对于由N个点组成的不同集合中，某集合对应的dichotomy最大，那么这个dichotomy值就是mH(H)，它的上界是2N



4. Break Point

positive rays      m(H) = N+1
positive internals       m(H) = 0.5N2 + 0.5N + 1
convex sets        m(H) = 2^N
2D perceptrons       m(H) < 2^N

其中，positive rays和positive intervals的成长函数都是polynomial的，如果用mH代替M的话，这两种情况是比较好的。而convex sets的成长函数是exponential的，即等于M，并不能保证机器学习的可行性。那么，对于2D perceptrons，它的成长函数究竟是polynomial的还是exponential的呢？

对于2D perceptrons，我们之前分析了3个点，可以做出8种所有的dichotomy，而4个点，就无法做出所有16个点的dichotomy了。所以，我们就把4称为2D perceptrons的break point（5、6、7等都是break point）。令有k个点，如果k大于等于break point时，它的成长函数一定小于2的k次方。

根据break point的定义，我们知道满足mH(k)≠2k的k的最小值就是break point。




第六课： Theory of Generalization

1. Restriction of Break Point
当N>k时，break point限制了mH(N)值的大小，也就是说影响成长函数mH(N)的因素主要有两个：

抽样数据集N

break point k（这个变量确定了假设的类型）

那么，如果给定N和k，能够证明其mH(N)的最大值的上界是多项式的，则根据霍夫丁不等式，就能用mH(N)代替M，得到机器学习是可行的。


2. Bounding Function: Basic Cases

Bound Function指的是当break point为k的时候，成长函数mH(N)可能的最大值。也就是说B(N,k)是mH(N)的上界，对应mH(N)最多有多少种dichotomy。那么，我们新的目标就是证明：

           B(N,k)≤poly(N)
这里值得一提的是，B(N,k)的引入不考虑是1D postive intrervals问题还是2D perceptrons问题，而只关心成长函数的上界是多少，从而简化了问题的复杂度。

当k=1时，B(N,1)恒为1。

当N < k时，根据break point的定义，很容易得到B(N,k)=2N。

当N = k时，此时N是第一次出现不能被shatter的值，所以最多只能有2N−1个dichotomies，则B(N,k)=2N−1


3. Bounding Function: Inductive Cases

首先，把B(4,3)所有情况写下来，共有11组。也就是说再加一种dichotomy，任意三点都能被shattered，11是极限。

对这11种dichotomy分组，目前分成两组，分别是orange和purple，orange的特点是，x1,x2和x3是一致的，x4不同并成对。

将Orange去掉x4后去重得到4个不同的vector并成为α，相应的purple为β。那么B(4,3)=2α+β，这个是直接转化。紧接着，由定义，B(4,3)是不能允许任意三点shatter的，所以由α和β构成的所有三点组合也不能shatter（alpha经过去重），即α+β≤B(3,3)。

另一方面，由于α中x4是成对存在的，且α是不能被任意三点shatter的，则能推导出α是不能被任意两点shatter的。这是因为，如果α是不能被任意两点shatter，而x4又是成对存在的，那么x1、x2、x3、x4组成的α必然能被三个点shatter。这就违背了条件的设定。这个地方的推导非常巧妙，也解释了为什么会这样分组。此处得到的结论是α≤B(3,2)

B(N, k) = 2a + b
a + b  <= B(N-1, k)
a      <= B(N-1, k-1)
B(N, k) <= B(N-1, k) + B(N-1, k-1)




第七课：  The VC Dimension

1. Definition of VC Dimension
首先，我们知道如果一个假设空间H有break point k，那么它的成长函数是有界的，它的上界称为Bound function。根据数学归纳法，Bound function也是有界的，且上界为
N^(k−1)
。从下面的表格可以看出，N^(k−1)比B(N,k)松弛很多。


有如下结论：
若假设空间H有break point k，且N足够大，则根据VC bound理论，算法有良好的泛化能力在假设空间中选择一个矩g，使Ein≈0
，则其在全集数据中的错误率会较低

VC Dimension就是某假设集H能够shatter的最多inputs的个数，即最大完全正确的分类能力。（注意，只要存在一种分布的inputs能够正确分类也满足）。
shatter的英文意思是“粉碎”，也就是说对于inputs的所有情况都能列举出来。例如对N个输入，如果能够将2N种情况都列出来，则称该N个输入能够被假设集H shatter。

根据之前breakpoint的定义：假设集不能被shatter任何分布类型的inputs的最少个数。则VC Dimension等于break point的个数减一。


2. VC Dimension of Perceptrons
回顾一下我们之前介绍的2D下的PLA算法，已知Perceptrons的k=4，即dvc=3。根据VC Bound理论，当N足够大的时候，Eout(g)≈Ein(g)。如果找到一个g，使Ein(g)≈0，那么就能证明PLA是可以学习的。

这是在2D情况下，那如果是多维的Perceptron，它对应的dvc又等于多少呢？

已知在1D Perceptron，dvc=2，在2D Perceptrons，dvc=3，那么我们有如下假设：dvc=d+1，其中d为维数。

首先证明第一个不等式：dvc≥d+1。

在d维里，我们只要找到某一类的d+1个inputs可以被shatter的话，那么必然得到dvc≥d+1。矩阵中，每一行代表一个inputs，每个inputs是d+1维的，共有d+1个inputs。这里构造的X很明显是可逆的。shatter的本质是假设空间H对X的所有情况的判断都是对的，即总能找到权重W，满足X∗W=y，W=X−1∗y。由于这里我们构造的矩阵X的逆矩阵存在，那么d维的所有inputs都能被shatter，也就证明了第一个不等式。

然后证明第二个不等式：dvc≤d+1。

在d维里，如果对于任何的d+2个inputs，一定不能被shatter，则不等式成立。我们构造一个任意的矩阵X，其包含d+2个inputs，该矩阵有d+1列，d+2行。这d+2个向量的某一列一定可以被另外d+1个向量线性表示，例如对于向量Xd+2，可表示为： 
Xd+2=a1∗X1+a2∗X2+⋯+ad+1∗Xd+1
其中，假设a1>0，a2,⋯,ad+1<0.

那么如果X1是正类，X2,⋯,Xd+1均为负类，则存在W，得到如下表达式： 
Xd+2∗W=a1∗X1∗W+a2∗X2∗W+⋯+ad+1∗Xd+1∗W>0
因为其中蓝色项大于0，代表正类；红色项小于0，代表负类。所有对于这种情况，Xd+2一定是正类，无法得到负类的情况。也就是说，d+2个inputs无法被shatter。



3. Physical Intuition VC Dimension

自由度是可以任意调节的，如同上图中的旋钮一样，可以调节。VC Dimension代表了假设空间的分类能力，即反映了H的自由度，产生dichotomy的数量，也就等于features的个数，但也不是绝对的。

例如，对2D Perceptrons，线性分类，dvc=3，则W={w0,w1,w2}，也就是说只要3个features就可以进行学习，自由度为3。


4. Interpreting VC Dimension

根据之前的泛化不等式，如果|Ein−Eout|>ϵ，即出现bad坏的情况的概率最大不超过δ。那么反过来，对于good好的情况发生的概率最小为1−δ。

ϵ 表现了假设空间H的泛化能力，ϵ越小，泛化能力越大。


所以可以得出结论：

dvc 越大，Ein越小，Ω越大（复杂）。

dvc越小，Ein越大，Ω越小（简单）。

随着dvc增大，Eout会先减小再增大。

所以，为了得到最小的Eout，不能一味地增大dvc以减小Ein，因为Ein太小的时候，模型复杂度会增加，造成Eout变大。也就是说，选择合适的dvc，选择的features个数要合适。


