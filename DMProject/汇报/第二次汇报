第八课： Noise and Error

1. Noise and Probablistic target

Data Sets的Noise一般有三种情况：

--由于人为因素，正类被误分为负类，或者负类被误分为正类；

--同样特征的样本被模型分为不同的类；

--样本的特征被错误记录和使用。

之前的数据集是确定的，即没有Noise的，叫做Deterministic。现在有Noise了，也就是说在某点处不再是确定分布，而是概率分布了，即对每个(x，y)出现的概率是P(y|x)。

因为Noise的存在，比如在x点，有0.7的概率y=1，有0.3的概率y=0，即y是按照P(y|x)分布的。数学上可以证明如果数据集按照P(y|x)概率分布且是iid的，那么以前证明机器可以学习的方法依然奏效，VC Dimension有限即可推断Ein和Eout是近似的。

P(y|x) 称之为目标分布（Target Distribution）。它实际上告诉我们最好的选择是什么，同时伴随着多少noise。其实，没有noise的数据仍然可以看成“特殊”的P(y|x)概率分布，即概率仅是1和0.对于以前确定的数据集： 
        P(y|x)=1,for y=f(x)

        P(y|x)=0,for y≠f(x)


2. ERROR Measure
机器学习需要考虑的问题是找出的矩g与目标函数f有多相近，之前一直使用Eout进行误差的估计，那一般的错误测量有哪些形式呢？

矩g对错误的衡量有三个特性：

--out-of-sample：样本外的未知数据

--pointwise：对每个数据点x进行测试

--classification：看prediction与target是否一致，classification error通常称为0/1 error

PointWise error实际上就是对数据集的每个点计算错误并计算平均，pointwise error一般可以分成两类：0/1 error和squared error。0/1 error通常用在分类（classification）问题上，而squared error通常用在回归（regression）问题上。


Ideal Mini-Target由P(y|x)和err共同决定，0/1 error和squared error的Ideal Mini-Target计算方法不一样。0/1 error中的mini-target是取P(y|x)最大的那个类，而squared error中的mini-target是取所有类的加权平方和。

有了错误衡量，就会知道当前的矩g是好还是不好，并会让演算法不断修正，得到更好的矩g，从而使得g与目标函数更接近。



3. Algorithmic Error Measure
Error有两种：false accept和false reject。false accept意思是误把负类当成正类，false reject是误把正类当成负类。 根据不同的机器学习问题，false accept和false reject应该有不同的权重，这根实际情况是符合的，比如是超市优惠，那么false reject应该设的大一些；如果是安保系统，那么false accept应该设的大一些。



4. Weighted Classification
实际上，机器学习的Cost Function即来自于这些error，也就是算法里面的迭代的目标函数，通过优化使得Error（Ein）不断变小。 
cost function中，false accept和false reject赋予不同的权重，在演算法中体现。对不同权重的错误惩罚，可以选用virtual copying的方法。




第九课： Linear Regression

1. 线性回归问题

令用户特征集为d维的X，加上常数项，维度为d+1，与权重w的线性组合即为Hypothesis,记为h(x)。线性回归的预测函数取值在整个实数空间，这跟线性分类不同。

        h(x)=wTX

在一维或者多维空间里，线性回归的目标是找到一条直线（对应一维）、一个平面（对应二维）或者更高维的超平面，使样本集中的点更接近它，也就是残留误差Residuals最小化。

一般最常用的错误测量方式是基于最小二乘法，其目标是计算误差的最小平方和对应的权重w，
        err(yhat, y) = (yhat - y)^2

线性最小二乘法的解是closed-form，即X=(ATA)−1ATy，而非线性最小二乘法没有closed-form，通常用迭代法求解。


2. 线性回归算法
样本数据误差Ein是权重w的函数，因为X和y都是已知的。我们的目标就是找出合适的w，使Ein能够最小。那么如何计算呢？

首先，运用矩阵转换的思想，将Ein计算转换为矩阵的形式。

对于此类线性回归问题，Ein(w)一般是个凸函数。凸函数的话，我们只要找到一阶导数等于零的位置，就找到了最优解。那么，我们将Ew对每个wi,i=0,1,⋯,d求偏导，偏导为零的wi，即为最优化的权重值分布。

        w = (xTx)^-1XTy

最终，我们推导得到了权重向量w=(XTX)−1XTy，这是上文提到的closed-form解。其中，(XTX)−1XT又称为伪逆矩阵pseudo-inverse，记为X+，维度是(d+1)xN。


3. 泛化问题
现在，可能有这样一个疑问，就是这种求解权重向量的方法是机器学习吗？或者说这种方法满足我们之前推导VC Bound，即是否泛化能力强Ein≈Eout？

y是N维空间的一个向量，粉色区域表示输入矩阵X乘以不同权值向量w所构成的空间，根据所有w的取值，预测输出都被限定在粉色的空间中。向量y^就是粉色空间中的一个向量，代表预测的一种。y是实际样本数据输出值。

机器学习的目的是在粉色空间中找到一个y^，使它最接近真实的y，那么我们只要将y在粉色空间上作垂直投影即可，投影得到的y^即为在粉色空间内最接近y的向量。这样即使平均误差E¯¯¯¯最小。

从图中可以看出，y^是y的投影，已知y^=Hy，那么H表示的就是将y投影到y^的一种操作。图中绿色的箭头y−y^是向量y与y^相减，y−y^垂直于粉色区域。已知(I−H)y=y−y^那么I-H表示的就是将y投影到y−y^即垂直于粉色区域的一种操作。这样的话，我们就赋予了H和I-H不同但又有联系的物理意义。

这里trace(I-H)称为I-H的迹，值为N-(d+1)。这条性质很重要，一个矩阵的 trace等于该矩阵的所有特征值(Eigenvalues)之和。下面给出简单证明：

trace(I−H)=trace(I)−trace(H) 
=N−trace(XX+)=N−trace(X(XTX)−1XT 
=N−trace(XTX(XTX)−1)=N−trace(Id+1) 
=N−(d+1)
介绍下该I-H这种转换的物理意义：原来有一个有N个自由度的向量y，投影到一个有d+1维的空间x（代表一列的自由度，即单一输入样本的参数，如图中粉色区域），而余数剩余的自由度最大只有N-(d+1)种。


由上面推导，已知向量y经过I-H转换为y−y^，而noise与y是线性变换关系，那么根据线性函数知识，我们推导出noise经过I-H也能转换为y−y^。则对于样本平均误差，有下列推导成立：

Ein(wLIN)=1N||y−y^||2=1N||(I−H)noise||2=1N(N−(d+1))||noise||2
即

Ein=noiselevel∗(1−d+1N)
同样，对Eout有如下结论：

Eout=noiselevel∗(1+d+1N)

Ein 与Eout形式上只差了(d+1)N项，Ein是我们看得到的样本的平均误差，如果有noise，把预测往noise那边偏一点，让Ein好看一点点，所以减去(d+1)N项。那么同时，新的样本Eout是我们看不到的，如果noise在反方向，那么Eout就应该加上(d+1)N项。 







第十课   Logistic Regression

1. Logistic regression problem

此时更关心的是目标函数的值（分布在0,1之间），表示是正类的概率。这跟我们原来讨论的二分类问题不太一样，我们把这个问题称为软性二分类问题（’soft’ binary classification）。这个值越接近1，表示正类的可能性越大；越接近0，表示负类的可能性越大。

对于软性二分类问题，理想的数据是分布在[0,1]之间的具体值，但是实际中的数据只可能是0或者1，我们可以把实际中的数据看成是理想数据加上了噪声的影响。

如果目标函数是f(x)=P(+1|x)∈[0,1]的话，如何找到一个好的Hypothesis跟这个目标函数很接近呢？

首先，根据之前的做法，对所有的特征值进行加权处理。

Sigmoid Function函数记为θ(s)=1/1+e−s，满足θ(−∞)=0，θ(0)=12，θ(+∞)=1。这个函数是平滑的、单调的S型函数。则对于逻辑回归问题，hypothesis就是这样的形式：

        h(x)=1/(1+e−wTx)
那我们的目标就是求出这个预测函数h(x)，使它接近目标函数f(x)。


2. Logistic Regression Error

将Logistic Regression与之前讲的Linear Classification、Linear Regression做个比较：

这三个线性模型都会用到线性scoring function s=wTx。linear classification的误差使用的是0/1 err；linear regression的误差使用的是squared err。那么logistic regression的误差该如何定义呢？

先介绍一下“似然性”的概念。目标函数f(x)=P(+1|x)，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。

logistic function: h(x)=θ(wTx)满足一个性质：1−h(x)=h(−x)。那么，似然性h:

        likelihood(h)=P(x1)h(+x1)×P(x2)h(−x2)×⋯P(xN)h(−xN)
因为P(xn)对所有的h来说，都是一样的，所以我们可以忽略它。那么我们可以得到logistic h正比于所有的h(ynx)乘积。我们的目标就是让乘积值最大化。

至此，得到了logistic regression的err function，称之为cross-entropy error交叉熵误差：

       err = ln(1+exp(-ywx))


3. Gradient of Logistic Regression Error
已经推导了Ein的表达式，那接下来的问题就是如何找到合适的向量w，让Ein最小。

为了计算Ein最小值，我们就要找到让∇Ein(w)等于0的位置。

要求θ(−ynwTxn)与−ynxn的线性加权和为0，那么一种情况是线性可分，如果所有的权重θ(−ynwTxn)为0，那就能保证∇Ein(w)为0。θ(−ynwTxn)是sigmoid function，根据其特性，只要让−ynwTxn≪0，即ynwTxn≫0。ynwTxn≫0表示对于所有的点，yn与wTxn都是同号的，这表示数据集D必须是全部线性可分的才能成立。

然而，保证所有的权重θ(−ynwTxn)为0是不太现实的，总有不等于0的时候，那么另一种常见的情况是非线性可分，只能通过使加权和为零，来求解w。这种情况没有closed-form解，与Linear Regression不同，只能用迭代方法求解。

w每次更新包含两个内容：一个是每次更新的方向ynxn，用v表示，另一个是每次更新的步长η。参数(v,η)和终止条件决定了我们的迭代优化算法。

4. Gradient Descent

把Ein(w)曲线看做是一个山谷的话，要求Ein(w)最小，即可比作下山的过程。整个下山过程由两个因素影响：一个是下山的单位方向v；另外一个是下山的步长η。

利用微分思想和线性近似，假设每次下山我们只前进一小步，即η很小，那么根据泰勒Taylor一阶展开，可以得到： 
Ein(wt+ηv)≈Ein(wt)+ηvT∇Ein(wt)

迭代的目的是让Ein越来越小，即让Ein(wt+ηv)<Ein(wt)。η是标量，因为如果两个向量方向相反的话，那么他们的内积最小（为负），也就是说如果方向v与梯度∇Ein(wt)反向的话，那么就能保证每次迭代Ein(wt+ηv)<Ein(wt)都成立。

对学习速率η做个更修正，梯度下降算法的迭代公式可以写成： 
       wt+1←wt−η′∇Ein(wt)


总结一下基于梯度下降的Logistic Regression算法步骤如下：

--初始化w0
--计算梯度∇Ein(wt)=1/N∑Nn=1θ(−ynwTtxn)(−ynxn)
--迭代跟新wt+1←wt−η∇Ein(wt)
--满足∇Ein(wt+1)≈0或者达到迭代次数，迭代结束



