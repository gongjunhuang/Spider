第八课： Noise and Error

1. Noise and Probablistic target

Data Sets的Noise一般有三种情况：

--由于人为因素，正类被误分为负类，或者负类被误分为正类；

--同样特征的样本被模型分为不同的类；

--样本的特征被错误记录和使用。

之前的数据集是确定的，即没有Noise的，叫做Deterministic。现在有Noise了，也就是说在某点处不再是确定分布，而是概率分布了，即对每个(x，y)出现的概率是P(y|x)。

因为Noise的存在，比如在x点，有0.7的概率y=1，有0.3的概率y=0，即y是按照P(y|x)分布的。数学上可以证明如果数据集按照P(y|x)概率分布且是iid的，那么以前证明机器可以学习的方法依然奏效，VC Dimension有限即可推断Ein和Eout是近似的。

P(y|x) 称之为目标分布（Target Distribution）。它实际上告诉我们最好的选择是什么，同时伴随着多少noise。其实，没有noise的数据仍然可以看成“特殊”的P(y|x)概率分布，即概率仅是1和0.对于以前确定的数据集： 
        P(y|x)=1,for y=f(x)

        P(y|x)=0,for y≠f(x)


2. ERROR Measure
机器学习需要考虑的问题是找出的矩g与目标函数f有多相近，之前一直使用Eout进行误差的估计，那一般的错误测量有哪些形式呢？

矩g对错误的衡量有三个特性：

--out-of-sample：样本外的未知数据

--pointwise：对每个数据点x进行测试

--classification：看prediction与target是否一致，classification error通常称为0/1 error

PointWise error实际上就是对数据集的每个点计算错误并计算平均，pointwise error一般可以分成两类：0/1 error和squared error。0/1 error通常用在分类（classification）问题上，而squared error通常用在回归（regression）问题上。


Ideal Mini-Target由P(y|x)和err共同决定，0/1 error和squared error的Ideal Mini-Target计算方法不一样。0/1 error中的mini-target是取P(y|x)最大的那个类，而squared error中的mini-target是取所有类的加权平方和。

有了错误衡量，就会知道当前的矩g是好还是不好，并会让演算法不断修正，得到更好的矩g，从而使得g与目标函数更接近。



3. Algorithmic Error Measure
Error有两种：false accept和false reject。false accept意思是误把负类当成正类，false reject是误把正类当成负类。 根据不同的机器学习问题，false accept和false reject应该有不同的权重，这根实际情况是符合的，比如是超市优惠，那么false reject应该设的大一些；如果是安保系统，那么false accept应该设的大一些。



4. Weighted Classification
实际上，机器学习的Cost Function即来自于这些error，也就是算法里面的迭代的目标函数，通过优化使得Error（Ein）不断变小。 
cost function中，false accept和false reject赋予不同的权重，在演算法中体现。对不同权重的错误惩罚，可以选用virtual copying的方法。




第九课： Linear Regression

1. 线性回归问题

令用户特征集为d维的X，加上常数项，维度为d+1，与权重w的线性组合即为Hypothesis,记为h(x)。线性回归的预测函数取值在整个实数空间，这跟线性分类不同。

        h(x)=wTX

在一维或者多维空间里，线性回归的目标是找到一条直线（对应一维）、一个平面（对应二维）或者更高维的超平面，使样本集中的点更接近它，也就是残留误差Residuals最小化。

一般最常用的错误测量方式是基于最小二乘法，其目标是计算误差的最小平方和对应的权重w，
        err(yhat, y) = (yhat - y)^2

线性最小二乘法的解是closed-form，即X=(ATA)−1ATy，而非线性最小二乘法没有closed-form，通常用迭代法求解。


2. 线性回归算法
样本数据误差Ein是权重w的函数，因为X和y都是已知的。我们的目标就是找出合适的w，使Ein能够最小。那么如何计算呢？

首先，运用矩阵转换的思想，将Ein计算转换为矩阵的形式。

对于此类线性回归问题，Ein(w)一般是个凸函数。凸函数的话，我们只要找到一阶导数等于零的位置，就找到了最优解。那么，我们将Ew对每个wi,i=0,1,⋯,d求偏导，偏导为零的wi，即为最优化的权重值分布。

        w = (xTx)^-1XTy

最终，我们推导得到了权重向量w=(XTX)−1XTy，这是上文提到的closed-form解。其中，(XTX)−1XT又称为伪逆矩阵pseudo-inverse，记为X+，维度是(d+1)xN。


3. 泛化问题
现在，可能有这样一个疑问，就是这种求解权重向量的方法是机器学习吗？或者说这种方法满足我们之前推导VC Bound，即是否泛化能力强Ein≈Eout？

y是N维空间的一个向量，粉色区域表示输入矩阵X乘以不同权值向量w所构成的空间，根据所有w的取值，预测输出都被限定在粉色的空间中。向量y^就是粉色空间中的一个向量，代表预测的一种。y是实际样本数据输出值。

机器学习的目的是在粉色空间中找到一个y^，使它最接近真实的y，那么我们只要将y在粉色空间上作垂直投影即可，投影得到的y^即为在粉色空间内最接近y的向量。这样即使平均误差E¯¯¯¯最小。

从图中可以看出，y^是y的投影，已知y^=Hy，那么H表示的就是将y投影到y^的一种操作。图中绿色的箭头y−y^是向量y与y^相减，y−y^垂直于粉色区域。已知(I−H)y=y−y^那么I-H表示的就是将y投影到y−y^即垂直于粉色区域的一种操作。这样的话，我们就赋予了H和I-H不同但又有联系的物理意义。

这里trace(I-H)称为I-H的迹，值为N-(d+1)。这条性质很重要，一个矩阵的 trace等于该矩阵的所有特征值(Eigenvalues)之和。下面给出简单证明：

trace(I−H)=trace(I)−trace(H) 
=N−trace(XX+)=N−trace(X(XTX)−1XT 
=N−trace(XTX(XTX)−1)=N−trace(Id+1) 
=N−(d+1)
介绍下该I-H这种转换的物理意义：原来有一个有N个自由度的向量y，投影到一个有d+1维的空间x（代表一列的自由度，即单一输入样本的参数，如图中粉色区域），而余数剩余的自由度最大只有N-(d+1)种。


由上面推导，已知向量y经过I-H转换为y−y^，而noise与y是线性变换关系，那么根据线性函数知识，我们推导出noise经过I-H也能转换为y−y^。则对于样本平均误差，有下列推导成立：

Ein(wLIN)=1N||y−y^||2=1N||(I−H)noise||2=1N(N−(d+1))||noise||2
即

Ein=noiselevel∗(1−d+1N)
同样，对Eout有如下结论：

Eout=noiselevel∗(1+d+1N)

Ein 与Eout形式上只差了(d+1)N项，Ein是我们看得到的样本的平均误差，如果有noise，把预测往noise那边偏一点，让Ein好看一点点，所以减去(d+1)N项。那么同时，新的样本Eout是我们看不到的，如果noise在反方向，那么Eout就应该加上(d+1)N项。 







第十课   Logistic Regression

1. Logistic regression problem

此时更关心的是目标函数的值（分布在0,1之间），表示是正类的概率。这跟我们原来讨论的二分类问题不太一样，我们把这个问题称为软性二分类问题（’soft’ binary classification）。这个值越接近1，表示正类的可能性越大；越接近0，表示负类的可能性越大。

对于软性二分类问题，理想的数据是分布在[0,1]之间的具体值，但是实际中的数据只可能是0或者1，我们可以把实际中的数据看成是理想数据加上了噪声的影响。

如果目标函数是f(x)=P(+1|x)∈[0,1]的话，如何找到一个好的Hypothesis跟这个目标函数很接近呢？

首先，根据之前的做法，对所有的特征值进行加权处理。

Sigmoid Function函数记为θ(s)=1/1+e−s，满足θ(−∞)=0，θ(0)=12，θ(+∞)=1。这个函数是平滑的、单调的S型函数。则对于逻辑回归问题，hypothesis就是这样的形式：

        h(x)=1/(1+e−wTx)
那我们的目标就是求出这个预测函数h(x)，使它接近目标函数f(x)。


2. Logistic Regression Error

将Logistic Regression与之前讲的Linear Classification、Linear Regression做个比较：

这三个线性模型都会用到线性scoring function s=wTx。linear classification的误差使用的是0/1 err；linear regression的误差使用的是squared err。那么logistic regression的误差该如何定义呢？

先介绍一下“似然性”的概念。目标函数f(x)=P(+1|x)，如果我们找到了hypothesis很接近target function。也就是说，在所有的Hypothesis集合中找到一个hypothesis与target function最接近，能产生同样的数据集D，包含y输出label，则称这个hypothesis是最大似然likelihood。

logistic function: h(x)=θ(wTx)满足一个性质：1−h(x)=h(−x)。那么，似然性h:

        likelihood(h)=P(x1)h(+x1)×P(x2)h(−x2)×⋯P(xN)h(−xN)
因为P(xn)对所有的h来说，都是一样的，所以我们可以忽略它。那么我们可以得到logistic h正比于所有的h(ynx)乘积。我们的目标就是让乘积值最大化。

至此，得到了logistic regression的err function，称之为cross-entropy error交叉熵误差：

       err = ln(1+exp(-ywx))


3. Gradient of Logistic Regression Error
已经推导了Ein的表达式，那接下来的问题就是如何找到合适的向量w，让Ein最小。

为了计算Ein最小值，我们就要找到让∇Ein(w)等于0的位置。

要求θ(−ynwTxn)与−ynxn的线性加权和为0，那么一种情况是线性可分，如果所有的权重θ(−ynwTxn)为0，那就能保证∇Ein(w)为0。θ(−ynwTxn)是sigmoid function，根据其特性，只要让−ynwTxn≪0，即ynwTxn≫0。ynwTxn≫0表示对于所有的点，yn与wTxn都是同号的，这表示数据集D必须是全部线性可分的才能成立。

然而，保证所有的权重θ(−ynwTxn)为0是不太现实的，总有不等于0的时候，那么另一种常见的情况是非线性可分，只能通过使加权和为零，来求解w。这种情况没有closed-form解，与Linear Regression不同，只能用迭代方法求解。

w每次更新包含两个内容：一个是每次更新的方向ynxn，用v表示，另一个是每次更新的步长η。参数(v,η)和终止条件决定了我们的迭代优化算法。

4. Gradient Descent

把Ein(w)曲线看做是一个山谷的话，要求Ein(w)最小，即可比作下山的过程。整个下山过程由两个因素影响：一个是下山的单位方向v；另外一个是下山的步长η。

利用微分思想和线性近似，假设每次下山我们只前进一小步，即η很小，那么根据泰勒Taylor一阶展开，可以得到： 
Ein(wt+ηv)≈Ein(wt)+ηvT∇Ein(wt)

迭代的目的是让Ein越来越小，即让Ein(wt+ηv)<Ein(wt)。η是标量，因为如果两个向量方向相反的话，那么他们的内积最小（为负），也就是说如果方向v与梯度∇Ein(wt)反向的话，那么就能保证每次迭代Ein(wt+ηv)<Ein(wt)都成立。

对学习速率η做个更修正，梯度下降算法的迭代公式可以写成： 
       wt+1←wt−η′∇Ein(wt)


总结一下基于梯度下降的Logistic Regression算法步骤如下：

--初始化w0
--计算梯度∇Ein(wt)=1/N∑Nn=1θ(−ynwTtxn)(−ynxn)
--迭代跟新wt+1←wt−η∇Ein(wt)
--满足∇Ein(wt+1)≈0或者达到迭代次数，迭代结束






十二课： Nonlinear Transformation

1. Quadratic Hypothesis

之前介绍的线性模型，在2D平面上是一条直线，在3D空间中是一个平面。数学上，我们用线性得分函数s来表示：s=wTx。其中，x为特征值向量，w为权重，s是线性的。

线性模型的优点就是，它的VC Dimension比较小，保证了Ein≈Eout。但是缺点也很明显，对某些非线性问题，可能会造成Ein很大，虽然Ein≈Eout，但是也造成Eout很大，分类效果不佳。

为了解决线性模型的缺点，可以使用非线性模型来进行分类。例如数据集D不是线性可分的，而是圆形可分的，圆形内部是正类，外面是负类。假设它的hypotheses可以写成： 
        hSEP(x)=sign(−x1^2−x2^2+0.6)

上面介绍的平面圆形分类例子，它的h(x)的权重w0=0.6，w1=-1，w2=-1，但是h(x)的特征不是线性模型的(1,x1,x2)，而是(1,x1^2,x2^2)。我们令z0=1，z1=x1^2，z2=x2^2，那么，h(x)变成：

h(x)=sign(w˘0⋅z0+w˘1⋅z1+w˘2⋅z2)=sign(0.6⋅z0−1⋅z1−1⋅z2)=sign(w˘Tz)

这种xn→zn的转换可以看成是x空间的点映射到z空间中去，而在z域中，可以用一条直线进行分类，也就是从x空间的圆形可分映射到z空间的线性可分。z域中的直线对应于x域中的圆形。因此，我们把xn→zn这个过程称之为特征转换（Feature Transform）。通过这种特征转换，可以将非线性模型转换为另一个域中的线性模型。


已知x域中圆形可分在z域中是线性可分的，那么反过来，如果在z域中线性可分，是否在x域中一定是圆形可分的呢？答案是否定的。由于权重向量w取值不同，x域中的hypothesis可能是圆形、椭圆、双曲线等等多种情况。

对于二次hypothesis，它包含二次项、一次项和常数项1，z域中每一条线对应x域中的某二次曲线的分类方式，也许是圆，也许是椭圆，也许是双曲线等等。


2. Nonlinear Transform

如何设计一个好的二次hypothesis来达到良好的分类效果,目标就是在z域中设计一个最佳的分类线。

利用映射变换的思想，通过映射关系，把x域中的最高阶二次的多项式转换为z域中的一次向量，也就是从quardratic hypothesis转换成了perceptrons问题。用z值代替x多项式，其中向量z的个数与x域中x多项式的个数一致（包含常数项）。这样就可以在z域中利用线性分类模型进行分类训练。训练好的线性模型之后，再将z替换为x的多项式就可以了。

整个过程就是通过映射关系，换个空间去做线性分类，重点包括两个：

--特征转换

--训练线性模型


3. Price of Nonlinear Transform

若x特征维度是d维的，也就是包含d个特征，那么二次多项式个数，即z域特征维度是： 
        d˘=1+C1d+C2d+d=d(d+3)2+1

如果x特征维度是2维的，即(x1,x2)，那么它的二次多项式为(1,x1,x2,x1^2,x1x2,x2^2)，有6个。
现在，如果阶数更高，假设阶数为Q，那么对于x特征维度是d维的，它的z域特征维度为： 
        d˘=CQQ+d=CdQ+d=O(Qd)

由上式可以看出，计算z域特征维度个数的时间复杂度是Q的d次方，随着Q和d的增大，计算量会变得很大。同时，空间复杂度也大。也就是说，这种特征变换的一个代价是计算的时间、空间复杂度都比较大。


另一方面，z域中特征个数随着Q和d增加变得很大，同时权重w也会增大，即自由度增加，VC Dimension增大。令z域中的特征维度是1+d˘，则在在域中，任何d˘+2的输入都不能被shattered；同样，在x域中，任何d˘+2的输入也不能被shattered。d˘+1是VC Dimension的上界，如果d˘+1很大的时候，相应的VC Dimension就会很大。VC Dimension过大，模型的泛化能力会比较差。



如何选择合适的Q，来保证不会出现过拟合问题，使模型的泛化能力强呢？一般情况下，为了尽量减少特征自由度，我们会根据训练样本的分布情况，人为地减少、省略一些项。但是，这种人为地删减特征会带来一些“自我分析”代价，虽然对训练样本分类效果好，但是对训练样本外的样本，不一定效果好。所以，一般情况下，还是要保存所有的多项式特征，避免对训练样本的人为选择。


4. Structured Hypothesis Sets

首先，如果特征维度只有1维的话，那么变换多项式只有常数项：

        Φ0(x)=(1)
 
如果特征维度是两维的，变换多项式包含了一维的Φ0(x)：

        Φ1(x)=(Φ0(x),x1,x2,…,xd)
如果特征维度是三维的，变换多项式包含了二维的Φ1(x)：

        Φ2(x)=(Φ1(x),x21,x1x2,…,x2d)
以此类推，如果特征维度是Q次，那么它的变换多项式为：

        ΦQ(x)=(ΦQ−1(x),xQ1,xQ−11x2,⋯,xQd)
那么对于不同阶次构成的hypothesis有如下关系：

HΦ0⊂HΦ1⊂HΦ2⊂⋯⊂HΦQ
我们把这种结构叫做Structured Hypothesis Sets

那么对于这种Structured Hypothesis Sets，它们的VC Dimension满足下列关系：

dVC(H0)≤dVC(H1)≤dVC(H2)≤⋯≤dVC(HQ)
它的Ein满足下列关系：

Ein(g0)≥Ein(g1)≥Ein(g2)≥⋯≥Ein(gQ)


随着变换多项式的阶数增大，虽然Ein逐渐减小，但是model complexity会逐渐增大，造成Eout很大，所以阶数不能太高。

那么，如果选择的阶数很大，确实能使Ein接近于0，但是泛化能力通常很差，我们把这种情况叫做tempting sin。所以，一般最合适的做法是先从低阶开始，如先选择一阶hypothesis，看看Ein是否很小，如果Ein足够小的话就选择一阶，如果Ein大的话，再逐渐增加阶数，直到满足要求为止。也就是说，尽量选择低阶的hypothes，这样才能得到较强的泛化能力。