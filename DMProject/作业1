一、有监督和无监督的比较
1. 监督学习

监督式学习（Supervised learning），是一个机器学习中的方法，可以由训练
资料中学到或建立一个模式，并依此模式推测新的实例。训练资料是由输入（通常是向量）
和预期输出所组成。函数的输出可以是一个连续的值（称为回归分析），或是预测一个
分类标签（称作分类）。

监督式学习有两种形态的模型。最一般的，监督式学习产生一个全域模型，会将输入
物件对应到预期输出。而另一种，则是将这种对应实作在一个区域模型。为了解决一个
给定的监督式学习的问题（手写辨识），必须考虑以下步骤：

1）决定训练资料的范例的形态。在做其它事前，应决定要使用哪种资料为范例。
2）搜集训练资料。这资料须要具有真实的特征。
3）决定学习函数的输入特征的表示法。学习函数的准确度与输入的数据如何表示是有很大的关联度。
4）决定要学习的函数和其对应的学习算法所使用的数据结构。
5）完成设计。在搜集到的资料上跑学习算法。

目前最广泛被使用的分类器有人工神经网络、支持向量机、最近邻居法、高斯混合模型、朴素贝叶斯方法、决策树和径向基函数分类。

2. 无监督式学习

无监督式学习(Unsupervised Learning )是人工智能网络的一种算法，其目的
是去对原始资料进行分类，以便了解资料内部结构。有别于监督式学习网络，无监督
式学习网络在学习时并不知道其分类结果是否正确，亦即没有受到监督式增强。
其特点是仅对此种网络提供输入范例，而它会自动从这些范例中找出其潜在类别规则
。当学习完毕并经测试后，也可以将之应用到新的案例上。

无监督学习里典型的例子就是聚类了。聚类的目的在于把相似的东西聚在一起，而我们
并不关心这一类是什么。因此，一个聚类算法通常只需要知道如何计算相似度就可以
开始工作了。




二、分类和回归的比较
输入变量与输出变量均为连续变量的预测问题是回归问题。回归用于预测输入变量和输出
变量之间的关系特别是当输入变量值发生变化之后，输出变量值随之发生变化。回归模型
是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价于函数拟合，选择一条
函数曲线使其很好地拟合已知数据且很好地预测未知数据。

输出变量为有限个离散变量的预测问题为分类问题。监督学习中，当输出变量Y取有限个
离散值时，预测问题就成为分类问题。这时，输入变量X可以是离散的，也可以是
连续的。分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效
的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。
常用分类算法包括：k近邻算法、感知机、朴素贝叶斯算法、决策树、支持向量机等。

三、什么是过拟合以及解决办法、交叉验证
过拟合（over-fitting）：一味追求提高对训练数据的预测能力，所选模型的复杂度往往
比真模型更高，这种现象称为过拟合。在模型学习时选择的参数过多，以至于出现对已知数据预测的
很好，但对未知数据预测的很差的现象。

解决办法主要进行最优的模型选择，选择复杂度适当的模型，已达到使测试误差最小的学习
目的。最常用的模型选择方法：正则化和交叉验证。

交叉验证：随机的将数据切分成三个部分，分别为训练集、测试集以及验证集。训练集用来
训练模型，验证集用于模型的选择，测试集用于最终对学习方法的评估。
交叉验证的使用思想是重复地使用数据：把给定的数据进行切分，将切分的数据集组合成训练集
和测试集，在此基础上反复地进行训练、测试以及模型选择。

交叉验证主要分为：简单交叉验证、S折交叉验证和留一交叉验证。


四、模型评估指标（精确率、召回率、F值、ROC、AUC）
精确率（precision）的公式是P = TP/(TP+FP),它计算
的是所有"正确被检索的item(TP)"占所有"实际被检索到的(TP+FP)"的比例.

召回率（recall）的公式是R = TP/ (TP+FN),它计算的是所有"正确被检索的item(TP)"
占所有"应该检索到的item(TP+FN)"的比例。

F1值就是精确值和召回率的调和均值，也就是2/F1 = 1/P + 1/R

ROC曲线：接收者操作特征曲线（receiver operating characteristic curve），
是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺
激的感受性。

AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会
大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在
0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明
哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。
