a.什么样的问题需要CRF模型

标注跟分类最大的区别就是：标注采的特征里面有上下文分类结果，这个结果你是不知道的，他在“分类”的时候是跟上下文一起"分类的"。因为你要确定这个词的分类得先知道上一个词的分类，所以这个得整句话的所有词一起解，没法一个词一个词解。

而分类是根据当前特征确定当前类别，分类的时候不需要考虑上下文的分类结果，但可以引入上下文的特征。

了让我们的分类器表现的更好，可以在标记数据的时候，可以考虑相邻数据的标记信息。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。

在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的，当然CRF不是唯一的选择，也有很多其他的词性标注方法。

b.从随机场到线性链条件随机场

随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词...)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。

马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。继续举十个词的句子词性标注的例子：　如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性有关。　

理解了马尔科夫随机场，再理解CRF就容易了。CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有X和Y两种变量，X一般是给定的，而Y一般是在给定X的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。在我们十个词的句子词性标注的例子中，X是词，Y是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。

对于CRF，我们给出准确的数学语言描述：设X与Y是随机变量，P(Y|X)是给定X时Y的条件概率分布，若随机变量Y构成的是一个马尔科夫随机场，则称条件概率分布P(Y|X)是条件随机场。

注意在CRF的定义中，我们并没有要求X和Y有相同的结构。而实现中，我们一般都假设X和Y有相同的结构，即:
X=(X1,X2,...Xn),Y=(Y1,Y2,...Yn)

我们一般考虑的结构：X和Y有相同的结构的CRF就构成了线性链条件随机场(Linear chain Conditional Random Fields,以下简称 linear-CRF)。

在我们的十个词的句子的词性标记中，词有十个，词性也是十个，因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个linear-CRF。

我们再来看看 linear-CRF的数学定义：

设X=(X1,X2,...Xn),Y=(Y1,Y2,...Yn)均为线性链表示的随机变量序列，在给定随机变量序列X的情况下，随机变量Y的条件概率分布P(Y|X)构成条件随机场，即满足马尔科夫性：
        P(Yi|X,Y1,Y2,...Yn)=P(Yi|X,Yi−1,Yi+1)
　　　　则称P(Y|X)为线性链条件随机场。

在linear-CRF中，特征函数分为两类，第一类是定义在Y节点上的节点特征函数，这类特征函数只和当前节点有关，记为：
sl(yi,x,i),l=1,2,...L
　　其中L是定义在该节点的节点特征函数的总个数，i是当前节点在序列的位置。

　　第二类是定义在Y上下文的局部特征函数，这类特征函数只和当前节点和上一个节点有关，记为：
tk(yi−1,yi,x,i),k=1,2,...K
　　其中K是定义在该节点的局部特征函数的总个数，i是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之间的特征函数，是因为我们的linear-CRF满足马尔科夫性。

　　无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设tk的权重系数是λk,sl的权重系数是μl,则linear-CRF由我们所有的tk,λk,sl,μl共同决定。

　　此时我们得到了linear-CRF的参数化形式如下：
P(y|x)=1Z(x)exp(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))
　　其中，Z(x)为规范化因子：
Z(x)=∑yexp(∑i,kλktk(yi−1,yi,x,i)+∑i,lμlsl(yi,x,i))
　　回到特征函数本身，每个特征函数定义了一个linear-CRF的规则，则其系数定义了这个规则的可信度。所有的规则和其可信度一起构成了我们的linear-CRF的最终的条件概率分布。

c.前向后向算法评估标记序列概率

linear-CRF第一个问题是评估，即给定 linear-CRF的条件概率分布P(y|x), 在给定输入序列x和输出序列y时，计算条件概率P(yi|x)和P(yi−1，yi|x)以及对应的期望. 本文接下来会详细讨论问题一。

linear-CRF第二个问题是学习，即给定训练数据集X和Y，学习linear-CRF的模型参数wk和条件概率Pw(y|x)，这个问题的求解比HMM的学习算法简单的多，普通的梯度下降法，拟牛顿法都可以解决。

linear-CRF第三个问题是解码，即给定 linear-CRF的条件概率分布P(y|x),和输入序列x, 计算使条件概率最大的输出序列y。类似于HMM，使用维特比算法可以很方便的解决这个问题。　

要计算条件概率P(yi|x)和P(yi−1，yi|x)，我们也可以使用和HMM类似的方法，使用前向后向算法来完成。首先我们来看前向概率的计算。

我们定义αi(yi|x)表示序列位置i的标记是yi时，在位置i之前的部分标记序列的非规范化概率。之所以是非规范化概率是因为我们不想加入一个不影响结果计算的规范化因子Z(x)在分母里面。

　这个式子定义了在给定yi−1时，从yi−1转移到yi的非规范化概率。

这样，我们很容易得到序列位置i+1的标记是yi+1时，在位置i+1之前的部分标记序列的非规范化概率αi+1(yi+1|x)的递推公式：
        αTi+1(yi+1|x)=αTi(yi|x)Mi+1(yi+1,yi|x)
    在起点处，我们定义：
    α0(y0|x)={1     y0=start
              0     else
    假设我们可能的标记总数是m, 则yi的取值就有m个，我们用αi(x)表示这m个值组成的前向向量如下：
        αi(x)=(αi(yi=1|x),αi(yi=2|x),...αi(yi=m|x))T
　　同时用矩阵Mi(x)表示由Mi(yi−1,yi|x)形成的m×m阶矩阵：
        Mi(x)=[Mi(yi−1,yi|x)]
　  这样递推公式可以用矩阵乘积表示：
        αTi+1(x)=αTi(x)Mi+1(x)

同样的。我们定义βi(yi|x)表示序列位置i的标记是yi时，在位置i之后的从i+1到n的部分标记序列的非规范化概率。

这样，我们很容易得到序列位置i+1的标记是yi+1时，在位置i之后的部分标记序列的非规范化概率βi(yi|x)的递推公式：
βi(yi|x)=Mi+1(yi,yi+1|x)βi+1(yi+1|x)
　　在终点处，我们定义：
    βn+1(yn+1|x)={1    yn+1=stop
                  0    else

如果用向量表示，则有：
        βi(x)=Mi+1(x)βi+1(x)
由于规范化因子Z(x)的表达式是：
        Z(x)=∑c=1mαn(yc|x)=∑c=1mβ1(yc|x)
也可以用向量来表示Z(x):
        Z(x)=αTn(x)∙1=1T∙β1(x)
其中，1是m维全1向量。


d.模型学习与维特比算法解码

在linear-CRF模型参数学习问题中，我们给定训练数据集X和对应的标记序列Y，K个特征函数fk(x,y)，需要学习linear-CRF的模型参数wk和条件概率Pw(y|x)，其中条件概率Pw(y|x)和模型参数wk满足一下关系：
        Pw(y|x)=P(y|x)=1Zw(x)exp∑k=1Kwkfk(x,y)=exp∑k=1Kwkfk(x,y)∑yexp∑k=1Kwkfk(x,y)
所以我们的目标就是求出所有的模型参数wk，这样条件概率Pw(y|x)可以从上式计算出来。

求解这个问题有很多思路，比如梯度下降法，牛顿法，拟牛顿法。同时，这个模型中Pw(y|x)的表达式和最大熵模型原理小结中的模型一样，也可以使用最大熵模型中使用的改进的迭代尺度法(improved iterative scaling, IIS)来求解。

在使用梯度下降法求解模型参数之前，我们需要定义我们的优化函数，一般极大化条件分布Pw(y|x)的对数似然函数如下：
        L(w)=log∏x,yPw(y|x)P¯(x,y)=∑x,yP¯(x,y)logPw(y|x)
其中P¯(x,y)为经验分布，可以从先验知识和训练集样本中得到,这点和最大熵模型类似。为了使用梯度下降法，我们现在极小化f(w)=−L(Pw)如下：
f(w)=−∑x,yP¯(x,y)logPw(y|x)=∑x,yP¯(x,y)logZw(x)−∑x,yP¯(x,y)∑k=1Kwkfk(x,y)=∑xP¯(x)logZw(x)−∑x,yP¯(x,y)∑k=1Kwkfk(x,y)=∑xP¯(x)log∑yexp∑k=1Kwkfk(x,y)−∑x,yP¯(x,y)∑k=1Kwkfk(x,y)(1)(2)(3)(4)
　　　　
对w求导可以得到：
        ∂f(w)∂w=∑x,yP¯(x)Pw(y|x)f(x,y)−∑x,yP¯(x,y)f(x,y)
有了w的导数表达书，就可以用梯度下降法来迭代求解最优的w了。注意在迭代过程中，每次更新w后，需要同步更新Pw(x,y),以用于下一次迭代的梯度计算。

维特比算法本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。由于在之前已详述维特比算法，这里就是做一个简略的流程描述。

对于我们linear-CRF中的维特比算法，我们的第一个局部状态定义为δi(l),表示在位置i标记l各个可能取值(1,2...m)对应的非规范化概率的最大值。之所以用非规范化概率是，规范化因子Z(x)不影响最大值的比较。根据δi(l)的定义，我们递推在位置i+1标记l的表达式为：
        δi+1(l)=max1≤j≤m{δi(j)+∑k=1Kwkfk(yi=j,yi+1=l,x,i)},l=1,2,...m

和HMM的维特比算法类似，我们需要用另一个局部状态Ψi+1(l)来记录使δi+1(l)达到最大的位置i的标记取值,这个值用来最终回溯最优解，Ψi+1(l)的递推表达式为：
Ψi+1(l)=argmax1≤j≤m{δi(j)+∑k=1Kwkfk(yi=j,yi+1=l,x,i)},l=1,2,...m


现在我们总结下 linear-CRF模型维特比算法流程：

输入：模型的K个特征函数，和对应的K个权重。观测序列x=(x1,x2,...xn),可能的标记个数m
输出：最优标记序列y*=(y*1,y*2,...y*n)
1) 初始化：
δ1(l)=∑k=1Kwkfk(y0=start,y1=l,x,i)},l=1,2,...m
Ψ1(l)=start,l=1,2,...m

2) 对于i=1,2...n−1,进行递推：
δi+1(l)=max1≤j≤m{δi(j)+∑k=1Kwkfk(yi=j,yi+1=l,x,i)},l=1,2,...m
Ψi+1(l)=argmax1≤j≤m{δi(j)+∑k=1Kwkfk(yi=j,yi+1=l,x,i)},l=1,2,...m
　　　　
3) 终止：
        y*n=argmax1≤j≤mδn(j)

4)回溯：
        y*i=Ψi+1(y*i+1),i=n−1,n−2,...1

最终得到最优标记序列y∗=(y∗1,y∗2,...y∗n)
