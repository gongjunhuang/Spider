1. 感知机模型

在机器学习中，感知机（perceptron）是二分类的线性分类模型，属于监督学习算法。
输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机对应于输入空间中将
实例划分为两类的分离超平面。感知机旨在求出该超平面，为求得超平面导入了基于误
分类的损失函数，利用梯度下降法 对损失函数进行最优化（最优化）。感知机的学习
算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的
感知机模型对新的实例进行预测的，因此属于判别模型。

定义：假设输入空间(特征向量)为X⊆Rn，输出空间为Y={-1, +1}。输入x∈X表示实例的
特征向量，对应于输入空间的点；输出y∈Y表示示例的类别。由输入空间到输出空间的函数
为f(x)=sign(w⋅x+b)称为感知机。其中，参数w叫做权值向量weight，b称为偏置bias。
w⋅x表示w和x的点积。

∑wixi=w1x1+w2x2+...+wnxn,
sign为符号函数，即f(x) = 1 if x >= 0 else f(x) = -1。

在二分类问题中，f(x)f(x)的值（+1或-1）用于分类xx为正样本（+1）还是负样本（-1）。
感知机是一种线性分类模型，属于判别模型。我们需要做的就是找到一个最佳的满足w⋅x+b=0
的w和b值，即分离超平面（separating hyperplane）。


2. 感知机模型的损失函数

损失函数的一个自然选择是误分类点的总数，但是这样的损失函数不是参数w,b的可导函数，
不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所使用的。
    (1/||w||)|wx+b|
其中||w||是w的L2范数。

所有点到超平面的总距离为−(1/||w||)∑yi(w∗x+b)，不考虑(1/||w||)，则感知机的
损失函数为
    L(w,b)=−∑yi(w∗x+b)
可以看出，损失函数L(w, b)是非负的，如果没有误分类点，损失函数值是0.误分类点越少，
误分类点离超平面越近，损失函数越小。


3. 感知机模型损失函数的优化方法、以及什么是梯度下降法

用梯度下降的方法，对参数w和b进行不断的迭代更新。具体来说，就是先任意选取一个超平面S，
对应的参数分别为w和b，当然现在是可以任意赋值的，比如说选取w为全为0的向量，的值为0。
然后用梯度下降不断地极小化损失函数。由于随机梯度下降（stochastic gradient
descent）的效率要高于批S量梯度下降（batch gradient descent），所以这里采用随机
梯度下降的方法，每次随机选取一个误分类点对w和b进行更新。

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写
出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是
(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。对于在点(x0,y0)的具体梯度向
量就是(∂f/∂x0, ∂f/∂y0)T.或者▽f(x0,y0)，如果是3个参数的向量梯度，就是
(∂f/∂x, ∂f/∂y，∂f/∂z)T,以此类推。

梯度的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),
在点(x0,y0)，沿着梯度向量的方向就是(∂f/∂x0, ∂f/∂y0)T的方向是f(x,y)增加最
快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着
梯度向量相反的方向，也就是 -(∂f/∂x0, ∂f/∂y0)T的方向，梯度减少最快，也就是
更加容易找到函数的最小值。所以为了得到最小化的函数，沿着梯度向量相反的方向，不断地
极小化梯度函数。

感知机随机梯度下降法的直观解释：对于感知机而言，当一个样本点被误分类之后，被误
分类的样本点会位于决策超平面的错误的一侧，这时感知机会调整w, b的值，使决策超平面向
该误分类点的一侧移动，移动的尺度由决定，每一次移动，都可以减少误分类点和决策
超平面之间的距离，直到决策超平面越过该误分类点，使其被正确分类为止。


4. 感知机模型算法过程

输入：训练数据集T={(x1, y1), (x2, y2),..., (xn, yn)}
输出：w， b，感知机模型f(x)=sign(w⋅x+b)
（1）选取初值w0， b0
（2）在训练集中选取数据（xi, yi）
（3）如果yi(wxi+b)<=0， 利用步长对w,b进行更新
（4）转至第二步，直到训练集中没有误分类点。


5. 感知机的对偶形式

感知机算法的对偶式，是针对感知机随机梯度下降法来设计的。其基本思想是：感知机的
原始式中， x是自变量， y是因变量， w, b是模型的参数，通俗点说，在感知机的原始
算法中，w, b是用参数来表示x和y的。对偶式就是要把这这个关系反过来，用样本x和y
的线性组合来表示参数和。
