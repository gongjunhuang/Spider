a.boosting与bagging区别

boosting:
Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。

Bagging:
Bagging的算法原理和 boosting不同，它的弱学习器之间没有依赖关系，可以并行生成.bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，就可以得到T个采样集，对于这T个采样集，可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

随机采样一般采用的是自助采样法（Bootstrap sampling）,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。

b.adaboost算法的基本思路

训练集样本是T={(x,y1),(x2,y2),...(xm,ym)}, 训练集的在第k个弱学习器的输出权重为
D(k)=(wk1,wk2,...wkm);w1i=1/m

首先看Adaboost的分类问题。
分类问题的误差率很好理解和计算。由于多元分类是二元分类的推广，这里假设是二元分类问题，输出为{-1，1}，则第k个弱分类器Gk(x)在训练集上的加权误差率为:
ek=P(Gk(xi)≠yi)=∑iwkiI(Gk(xi)≠yi)

接着看弱学习器权重系数,对于二元分类问题，第k个弱分类器Gk(x)的权重系数为:
αk=1/2log(1−ek)/ek

假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,...wkm)，则对应的第k+1个弱分类器的样本集权重系数为:
    wk+1,i=wki/ZKexp(−αkyiGk(xi))
这里Zk是规范化因子:
Zk=∑wkiexp(−αkyiGk(xi))

从wk+1,iwk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)<0，导致样本的权重在第k+1个弱分类器中增大，如果分类正确，则权重在第k+1个弱分类器中减少.

Adaboost分类采用的是加权平均法，最终的强分类器为:
f(x)=sign(∑αkGk(x))


Adaboost的回归问题

由于Adaboost的回归问题有很多变种，这里以Adaboost R2算法为准。
先看看回归问题的误差率的问题，对于第k个弱学习器，计算他在训练集上的最大误差:
    Ek=max|yi−Gk(xi)|i=1,2...m
然后计算每个样本的相对误差:
    eki=|yi−Gk(xi)|/Ek

最终得到第k个弱学习器的 误差率:ek=∑wkieki

弱学习器权重系数α: αk=ek/1−ek

对于更新更新样本权重D，第k+1个弱学习器的样本集权重系数为:wk+1,i=wki/Zk*α1^(−ekik)

最后是结合策略，和分类问题稍有不同，采用的是对加权的弱学习器取中位数的方法，最终的强回归器为:
f(x)=∑(ln1/αk)g(x)

c.adaboost分类问题的损失函数优化

Adaboost是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。

模型为加法模型指的是最终的强分类器是若干个弱分类器加权平均而得到的。

前向分步学习算法也好理解，我们的算法是通过一轮轮的弱学习器学习，利用前一个弱学习器的结果来更新后一个弱学习器的训练集权重。也就是说，第k-1轮的强学习器为：
    fk−1(x)=∑αiGi(x)
而第k轮的强学习器为
    fk(x)=∑αiGi(x)
上两式一比较可以得到
    fk(x)=fk−1(x)+αkGk(x)
可见强学习器的确是通过前向分步学习算法一步步而得到的。

Adaboost损失函数为指数函数，即定义损失函数为：
    argmin∑exp(−yifk(x))
利用前向分步学习算法的关系可以得到损失函数为：
    (αk,Gk(x))=argmin∑exp[(−yi)(fk−1(x)+αG(x))]
令wki′=exp(−yifk−1(x)), 它的值不依赖于α,Gα,G,因此与最小化无关，仅仅依赖于fk−1(x),随着每一轮迭代而改变。

将这个式子带入损失函数,损失函数转化为：
    argmin∑w′kiexp[−yiαG(x)]

首先，我们求Gk(x)，可以得到
    Gk(x)=argmin∑w′kiI(yi≠G(xi))

将Gk(x)带入损失函数，并对α求导，使其等于0，则就得到了:
    αk=1/2log(1−ek)/ek

最后看样本权重的更新。利用fk(x)=fk−1(x)+αkGk(x)和w′ki=exp(−yifk−1(x))，即可得：
    w′k+1,i=w′kiexp[−yiαkGk(x)]



d.GBDT的负梯度拟合

GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。

　　在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x)=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。

用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。第t轮的第i个样本的损失函数的负梯度表示为：
    rti=−[∂L(yi,f(xi)))/∂f(xi)]f(x)=ft−1(x)

利用(xi,rti)(i=1,2,..m),可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域Rtj,j=1,2,...,J。其中J为叶子节点的个数。
针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的的输出值ctj如下：
    ctj=argmin∑L(yi,ft−1(xi)+c)

这样我们就得到了本轮的决策树拟合函数如下：
    ht(x)=∑I(x∈Rtj)

从而本轮最终得到的强学习器的表达式如下：
    ft(x)=ft−1(x)+∑I(x∈Rtj)

通过损失函数的负梯度来拟合，找到了一种通用的拟合损失误差的办法，这样无论是分类问题还是回归问题，通过其损失函数的负梯度的拟合，就可以用GBDT来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。



e.GBDT回归和分类算法

回归算法：

输入是训练集样本T={(x,y1),(x2,y2),...(xm,ym)}， 最大迭代次数T, 损失函数L。

输出是强学习器f(x)

1) 初始化弱学习器   f0(x)=argmin∑L(yi,c)

2) 对迭代轮数t=1,2,...T有：
   　a)对样本i=1,2，...m，计算负梯度
     rti=−[∂L(yi,f(xi)))/∂f(xi)]f(x)=ft−1(x)

     b)利用(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,...,J。其中J为回归树t的叶子节点的个数:

     c) 对叶子区域j =1,2,..J,计算最佳拟合值:
     ctj=argmin∑L(yi,ft−1(xi)+c)

     d) 更新强学习器
         ft(x)=ft−1(x)+∑jI(x∈Rtj)

3) 得到强学习器f(x)的表达式
    f(x)=fT(x)=f0(x)+∑∑jI(x∈Rtj)




分类算法：

GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。

　　为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。

二元GBDT分类算法：
对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：
    L(y,f(x))=log(1+exp(−yf(x)))
    其中y∈{−1,+1}y∈{−1,+1}。则此时的负梯度误差为：
    rti=−[∂L(y,f(xi)))/∂f(xi)]f(x)=ft−1(x)=yi/(1+exp(yif(xi)))

    对于生成的决策树，我们各个叶子节点的最佳残差拟合值为
    ctj=argmin∑log(1+exp(−yi(ft−1(xi)+c)))

    用近似值代替为：
        ctj=∑rti/∑|rti|(1−|rti|)

多元的GBDT算法：
    假设类别数为K，则此时对数似然损失函数为：
    L(y,f(x))=−∑yklogpk(x)

    其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为：
    pk(x)=exp(fk(x))/∑exp(fl(x))

    集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为：
    rtil=−[∂L(yi,f(xi)))/∂f(xi)]fk(x)=fl,t−1(x)=yil−pl,t−1(xi)

    观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t−1轮预测概率的差值。
    
    对于生成的决策树，我们各个叶子节点的最佳残差拟合值为
    ctjl=argmin∑∑L(yk,ft−1,l(x)+∑cjlI(xi∈Rtj))

    由于上式比较难优化，我们一般使用近似值代替
        ctjl=K−1/K*∑rtil∑|rtil|(1−|rtil|)

