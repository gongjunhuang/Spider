a.弄清楚书本中HMM实例和包括的三个基本问题

对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：
Q={q1,q2,...,qN},V={v1,v2,...vM}
　　　　其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。

　对于一个长度为T的序列，I对应的状态序列, O是对应的观察序列，即：
I={i1,i2,...,iT},O={o1,o2,...oT}
　其中，任意一个隐藏状态it∈Q,任意一个观察状态ot∈V
　
HMM模型做了两个很重要的假设如下：

　　1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻t的隐藏状态是it=qi,在时刻t+1的隐藏状态是it+1=qj, 则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：
    aij=P(it+1=qj|it=qi)
 　
   这样aij可以组成马尔科夫链的状态转移矩阵A:
       A=[aij]N×N
　　　　
2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t的隐藏状态是it=qj, 而对应的观察状态为ot=vk, 则该时刻观察状态vk在隐藏状态qj下生成的概率为bj(k),满足：
      bj(k)=P(ot=vk|it=qj)
　　　　
    这样bj(k)可以组成观测状态生成的概率矩阵B:
      B=[bj(k)]N×M
　　　　
    除此之外，我们需要一组在时刻t=1的隐藏状态概率分布Π:
      Π=[π(i)]N其中π(i)=P(i1=qi)
　　　　
一个HMM模型，可以由隐藏状态初始概率分布Π, 状态转移概率矩阵A和观测状态概率矩阵B决定。Π,A决定状态序列，B决定观测序列。因此，HMM模型可以由一个三元组λ表示如下：
      λ=(A,B,Π)

HMM观测序列的生成

输入的是HMM的模型λ=(A,B,Π),观测序列的长度T
　　输出是观测序列O={o1,o2,...oT}
　　生成的过程如下：

　　1）根据初始状态概率分布Π生成隐藏状态i1
　　2) for t from 1 to T

　　　　a. 按照隐藏状态it的观测状态分布bit(k)生成观察状态ot
　　　　b. 按照隐藏状态it的状态转移概率分布aitit+1产生隐藏状态it+1
　　　　所有的ot一起形成观测序列O={o1,o2,...oT}

HMM模型一共有三个经典的问题需要解决：

　　1） 评估观察序列概率。即给定模型λ=(A,B,Π)和观测序列O={o1,o2,...oT}，计算在模型λ下观测序列O出现的概率P(O|λ)。这个问题的求解需要用到前向后向算法。

　　2）模型参数学习问题。即给定观测序列O={o1,o2,...oT}，估计模型λ=(A,B,Π)的参数，使该模型下观测序列的条件概率P(O|λ)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。

　　3）预测问题，也称为解码问题。即给定模型λ=(A,B,Π)和观测序列O={o1,o2,...oT}，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。

b.用前向和后向算法求HMM观测序列的概率

前向算法本质上属于动态规划的算法，也就是我们要通过找到局部状态递推的公式，这样一步步的从子问题的最优解拓展到整个问题的最优解。

　　在前向算法中，通过定义“前向概率”来定义动态规划的这个局部状态。什么是前向概率呢, 其实定义很简单：定义时刻t时隐藏状态为qi, 观测状态的序列为o1,o2,...ot的概率为前向概率。记为：
      αt(i)=P(o1,o2,...ot,it=qi|λ)
　　既然是动态规划，我们就要递推了，现在我们假设我们已经找到了在时刻t时各个隐藏状态的前向概率，现在我们需要递推出时刻t+1时各个隐藏状态的前向概率。

　　我们可以基于时刻t时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即αt(j)aji就是在时刻t观测到o1,o2,...ot，并且时刻t隐藏状态qj, 时刻t+1隐藏状态qi的概率。如果将下面所有的线对应的概率求和，即∑j=1Nαt(j)aji就是在时刻t观测到o1,o2,...ot，并且时刻t+1隐藏状态qi的概率。继续一步，由于观测状态ot+1只依赖于t+1时刻隐藏状态qi, 这样[∑j=1Nαt(j)aji]bi(ot+1)就是在在时刻t+1观测到o1,o2,...ot，ot+1，并且时刻t+1隐藏状态qi的概率。而这个概率，恰恰就是时刻t+1对应的隐藏状态i的前向概率，这样我们得到了前向概率的递推关系式如下：
      αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1)

我们的动态规划从时刻1开始，到时刻T结束，由于αT(i)表示在时刻T观测序列为o1,o2,...oT，并且时刻T隐藏状态qi的概率，我们只要将所有隐藏状态对应的概率相加，即∑i=1NαT(i)就得到了在时刻T观测序列为o1,o2,...oT的概率。

　 下面总结下前向算法。

　　输入：HMM模型λ=(A,B,Π)，观测序列O=(o1,o2,...oT)
　　输出：观测序列概率P(O|λ)
　　1) 计算时刻1的各个隐藏状态前向概率：
      α1(i)=πibi(o1),i=1,2,...N
　　2) 递推时刻2,3,...T时刻的前向概率：
      αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2,...N
　　3) 计算最终结果：
      P(O|λ)=∑i=1NαT(i)
　　从递推公式可以看出，我们的算法时间复杂度是O(TN2)，比暴力解法的时间复杂度O(TNT)少了几个数量级。


后向算法和前向算法非常类似，都是用的动态规划，唯一的区别是选择的局部状态不同，后向算法用的是“后向概率”，那么后向概率是如何定义的呢？

　　定义时刻t时隐藏状态为qi, 从时刻t+1到最后时刻T的观测状态的序列为ot+1,ot+2,...oT的概率为后向概率。记为：
      βt(i)=P(ot+1,ot+2,...oT|it=qi,λ)
　　后向概率的动态规划递推公式和前向概率是相反的。现在假设已经找到了在时刻t+1时各个隐藏状态的后向概率βt+1(j)，需要递推出时刻t时各个隐藏状态的后向概率。可以计算出观测状态的序列为ot+2,ot+3,...oT， t时隐藏状态为qi, 时刻t+1隐藏状态为qj的概率为aijβt+1(j), 接着可以得到观测状态的序列为ot+1,ot+2,...oT， t时隐藏状态为qi, 时刻t+1隐藏状态为qj的概率为aijbj(ot+1)βt+1(j), 则把下面所有线对应的概率加起来，可以得到观测状态的序列为ot+1,ot+2,...oT， t时隐藏状态为qi的概率为∑j=1Naijbj(ot+1)βt+1(j)，这个概率即为时刻t的后向概率。

这样得到了后向概率的递推关系式如下：
      βt(i)=∑j=1Naijbj(ot+1)βt+1(j)

现在总结下后向算法的流程,注意下和前向算法的相同点和不同点：

　　输入：HMM模型λ=(A,B,Π)，观测序列O=(o1,o2,...oT)
　　输出：观测序列概率P(O|λ)
　　1) 初始化时刻T的各个隐藏状态后向概率：
      βT(i)=1,i=1,2,...N
　　2) 递推时刻T−1,T−2,...1时刻的后向概率：
      βt(i)=∑j=1Naijbj(ot+1)βt+1(j),i=1,2,...N
　　3) 计算最终结果：
      P(O|λ)=∑i=1Nπibi(o1)β1(i)
　　此时我们的算法时间复杂度仍然是O(TN2)。

c.鲍姆-韦尔奇算法原理

鲍姆-韦尔奇算法原理使用的就是EM算法的原理，那么需要在E步求出联合分布P(O,I|λ)基于条件概率P(I|O,λ¯)的期望，其中λ¯为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数λ。接着不停的进行EM迭代，直到模型参数的值收敛为止。

　　首先来看看E步，当前模型参数为λ¯, 联合分布P(O,I|λ)基于条件概率P(I|O,λ¯)的期望表达式为：
      L(λ,λ¯)=∑IP(I|O,λ¯)logP(O,I|λ)
　　在M步，我们极大化上式，然后得到更新后的模型参数如下：　
      λ¯=argmaxλ∑IP(I|O,λ¯)logP(O,I|λ)
　　通过不断的E步和M步的迭代，直到λ¯收敛。下面来看看鲍姆-韦尔奇算法的推导过程。

我们的训练数据为{(O1,I1),(O2,I2),...(OD,ID)}，其中任意一个观测序列Od={o(d)1,o(d)2,...o(d)T},其对应的未知的隐藏状态序列表示为：Id={i(d)1,i(d)2,...i(d)T}
　　　　
首先看鲍姆-韦尔奇算法的E步，我们需要先计算联合分布P(O,I|λ)的表达式如下：
    P(O,I|λ)=∏d=1Dπi(d)1bi(d)1(o(d)1)ai(d)1i(d)2bi(d)2(o(d)2)...ai(d)T−1i(d)Tbi(d)T(o(d)T)
　　
    我们的E步得到的期望表达式为：
      L(λ,λ¯)=∑IP(I|O,λ¯)logP(O,I|λ)
　　在M步我们要极大化上式。由于P(I|O,λ¯)=P(I,O|λ¯)/P(O|λ¯),而P(O|λ¯)是常数，因此我们要极大化的式子等价于：
      λ¯=argmaxλ∑IP(O,I|λ¯)logP(O,I|λ)
　　　　
我们将上面P(O,I|λ)的表达式带入我们的极大化式子，得到的表达式如下：
      λ¯=argmaxλ∑d=1D∑IP(O,I|λ¯)(logπi1+∑t=1T−1logaitait+1+∑t=1Tbit(ot))
　我们的隐藏模型参数λ=(A,B,Π),因此下面我们只需要对上式分别对A,B,Π求导即可得到我们更新的模型参数λ¯¯¯　

 
    首先对模型参数Π的求导。由于Π只在上式中括号里的第一部分出现，因此我们对于Π的极大化式子为：
πi¯=argmaxπi1∑d=1D∑IP(O,I|λ¯)logπi1=argmaxπi∑d=1D∑i=1NP(O,i(d)1=i|λ¯)logπi
　　　　由于πi还满足∑i=1Nπi=1，因此根据拉格朗日子乘法，我们得到πi要极大化的拉格朗日函数为：
       argmaxπi∑d=1D∑i=1NP(O,i(d)1=i|λ¯¯¯)logπi+γ(∑i=1Nπi−1)
　　　　其中，γ为拉格朗日系数。上式对πi求偏导数并令结果为0， 我们得到：
      ∑d=1DP(O,i(d)1=i|λ¯¯¯)+γπi=0
　　令i分别等于从1到N，从上式可以得到N个式子，对这N个式子求和可得：
      ∑d=1DP(O|λ¯)+γ=0
　　从上两式消去γ,得到πi的表达式为：
πi=∑d=1DP(O,i(d)1=i|λ¯)∑d=1DP(O|λ¯)=∑d=1DP(O,i(d)1=i|λ¯)DP(O|λ¯)=∑d=1DP(i(d)1=i|O,λ¯)D=∑d=1DP(i(d)1=i|O(d),λ¯)D
　　利用我们在隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率里第二节中前向概率的定义可得：
    P(i(d)1=i|O(d),λ¯)=γ(d)1(i)
　　因此最终我们在M步πi的迭代公式为：
      πi=∑d=1Dγ(d)1(i)D
 

　　现在我们来看看A的迭代公式求法。方法和Π的类似。由于A只在最大化函数式中括号里的第二部分出现，而这部分式子可以整理为：
∑d=1D∑I∑t=1T−1P(O,I|λ¯)logaitait+1=∑d=1D∑i=1N∑j=1N∑t=1T−1P(O,i(d)t=i,i(d)t+1=j|λ¯)logaij
　　由于aij还满足∑j=1Naij=1。和求解πi类似，我们可以用拉格朗日子乘法并对aij求导，并令结果为0，可以得到aij的迭代表达式为：
aij=∑d=1D∑t=1T−1P(O(d),i(d)t=i,i(d)t+1=j|λ¯)∑d=1D∑t=1T−1P(O(d),i(d)t=i|λ¯)
　　利用前向后向算法评估观察序列概率里前向概率的定义和ξt(i,j)的定义可得们在M步aij的迭代公式为：
     aij=∑d=1D∑t=1T−1ξ(d)t(i,j)∑d=1D∑t=1T−1γ(d)t(i)
 

　　现在我们来看看B的迭代公式求法。方法和Π的类似。由于B只在最大化函数式中括号里的第三部分出现，而这部分式子可以整理为：
∑d=1D∑I∑t=1TP(O,I|λ¯)logbit(ot)=∑d=1D∑j=1N∑t=1TP(O,i(d)t=j|λ¯)logbj(ot)
　　由于bj(ot)还满足∑k=1Mbj(ot=vk)=1。和求解πi类似，我们可以用拉格朗日子乘法并对bj(k)求导，并令结果为0，得到bj(k)的迭代表达式为：
bj(k)=∑d=1D∑t=1TP(O,i(d)t=j|λ¯)I(o(d)t=vk)∑d=1D∑t=1TP(O,i(d)t=j|λ¯)
　　其中I(o(d)t=vk)当且仅当o(d)t=vk时为1，否则为0. 利用前向后向算法前向概率的定义可得bj(ot)的最终表达式为：
bj(k)=∑d=1D∑t=1,o(d)t=vkTγ(d)t(i)∑d=1D∑t=1Tγ(d)t(i)
　　有了πi,aij,bj(k)的迭代公式，我们就可以迭代求解HMM模型参数了。

d.维特比算法原理

维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。

　　既然是动态规划算法，那么就需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。

　　第一个局部状态是在时刻t隐藏状态为i所有可能的状态转移路径i1,i2,...it中的概率最大值。记为δt(i):
δt(i)=maxi1,i2,...it−1P(it=i,i1,i2,...it−1,ot,ot−1,...o1|λ),i=1,2,...N
　　由δt(i)的定义可以得到δ的递推表达式：
δt+1(i)=maxi1,i2,...itP(it+1=i,i1,i2,...it,ot+1,ot,...o1|λ)=max1≤j≤N[δt(j)aji]bi(ot+1)(1)(2)
　　第二个局部状态由第一个局部状态递推得到。我们定义在时刻t隐藏状态为i的所有单个状态转移路径(i1,i2,...,it−1,i)中概率最大的转移路径中第t−1个节点的隐藏状态为Ψt(i),其递推表达式可以表示为：
Ψt(i)=argmax1≤j≤N[δt−1(j)aji]
　　有了这两个局部状态，我们就可以从时刻0一直递推到时刻T，然后利用Ψt(i)记录的前一个最可能的状态节点回溯，直到找到最优的隐藏状态序列。

现在我们来总结下维特比算法的流程：

　　输入：HMM模型λ=(A,B,Π)，观测序列O=(o1,o2,...oT)
　　输出：最有可能的隐藏状态序列I∗={i∗1,i∗2,...i∗T}
　　1）初始化局部状态：
      δ1(i)=πibi(o1),i=1,2...N
      Ψ1(i)=0,i=1,2...N
　　2) 进行动态规划递推时刻t=2,3,...T时刻的局部状态：
      δt(i)=max1≤j≤N[δt−1(j)aji]bi(0t),i=1,2...N
      Ψt(i)=argmax1≤j≤N[δt−1(j)aji],i=1,2...N
　  3) 计算时刻T最大的δT(i),   即为最可能隐藏状态序列出现的概率。计算时刻T最大的Ψt(i),即为时刻T最可能的隐藏状态。
      P*=max1≤j≤NδT(i)
      i*T=argmax1≤j≤N[δT(i)]
　　4) 利用局部状态Ψ(i)开始回溯。对于t=T−1,T−2,...,1：
i∗t=Ψt+1(i∗t+1)
　　最终得到最有可能的隐藏状态序列I*={i*1,i*2,...i*T}