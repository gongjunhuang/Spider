a.弄清楚书本中HMM实例和包括的三个基本问题

对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：
Q={q1,q2,...,qN},V={v1,v2,...vM}
　　　　其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。

　对于一个长度为T的序列，I对应的状态序列, O是对应的观察序列，即：
I={i1,i2,...,iT},O={o1,o2,...oT}
　其中，任意一个隐藏状态it∈Q,任意一个观察状态ot∈V
　
HMM模型做了两个很重要的假设如下：

　　1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻t的隐藏状态是it=qi,在时刻t+1的隐藏状态是it+1=qj, 则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：
    aij=P(it+1=qj|it=qi)
 　
   这样aij可以组成马尔科夫链的状态转移矩阵A:
       A=[aij]N×N
　　　　
2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t的隐藏状态是it=qj, 而对应的观察状态为ot=vk, 则该时刻观察状态vk在隐藏状态qj下生成的概率为bj(k),满足：
      bj(k)=P(ot=vk|it=qj)
　　　　
    这样bj(k)可以组成观测状态生成的概率矩阵B:
      B=[bj(k)]N×M
　　　　
    除此之外，我们需要一组在时刻t=1的隐藏状态概率分布Π:
      Π=[π(i)]N其中π(i)=P(i1=qi)
　　　　
一个HMM模型，可以由隐藏状态初始概率分布Π, 状态转移概率矩阵A和观测状态概率矩阵B决定。Π,A决定状态序列，B决定观测序列。因此，HMM模型可以由一个三元组λ表示如下：
      λ=(A,B,Π)

HMM观测序列的生成

输入的是HMM的模型λ=(A,B,Π),观测序列的长度T
　　输出是观测序列O={o1,o2,...oT}
　　生成的过程如下：

　　1）根据初始状态概率分布Π生成隐藏状态i1
　　2) for t from 1 to T

　　　　a. 按照隐藏状态it的观测状态分布bit(k)生成观察状态ot
　　　　b. 按照隐藏状态it的状态转移概率分布aitit+1产生隐藏状态it+1
　　　　所有的ot一起形成观测序列O={o1,o2,...oT}

HMM模型一共有三个经典的问题需要解决：

　　1） 评估观察序列概率。即给定模型λ=(A,B,Π)和观测序列O={o1,o2,...oT}，计算在模型λ下观测序列O出现的概率P(O|λ)。这个问题的求解需要用到前向后向算法。

　　2）模型参数学习问题。即给定观测序列O={o1,o2,...oT}，估计模型λ=(A,B,Π)的参数，使该模型下观测序列的条件概率P(O|λ)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。

　　3）预测问题，也称为解码问题。即给定模型λ=(A,B,Π)和观测序列O={o1,o2,...oT}，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法。

b.用前向和后向算法求HMM观测序列的概率

前向算法本质上属于动态规划的算法，也就是我们要通过找到局部状态递推的公式，这样一步步的从子问题的最优解拓展到整个问题的最优解。

　　在前向算法中，通过定义“前向概率”来定义动态规划的这个局部状态。什么是前向概率呢, 其实定义很简单：定义时刻t时隐藏状态为qi, 观测状态的序列为o1,o2,...ot的概率为前向概率。记为：
      αt(i)=P(o1,o2,...ot,it=qi|λ)
　　既然是动态规划，我们就要递推了，现在我们假设我们已经找到了在时刻t时各个隐藏状态的前向概率，现在我们需要递推出时刻t+1时各个隐藏状态的前向概率。

　　我们可以基于时刻t时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即αt(j)aji就是在时刻t观测到o1,o2,...ot，并且时刻t隐藏状态qj, 时刻t+1隐藏状态qi的概率。如果将下面所有的线对应的概率求和，即∑j=1Nαt(j)aji就是在时刻t观测到o1,o2,...ot，并且时刻t+1隐藏状态qi的概率。继续一步，由于观测状态ot+1只依赖于t+1时刻隐藏状态qi, 这样[∑j=1Nαt(j)aji]bi(ot+1)就是在在时刻t+1观测到o1,o2,...ot，ot+1，并且时刻t+1隐藏状态qi的概率。而这个概率，恰恰就是时刻t+1对应的隐藏状态i的前向概率，这样我们得到了前向概率的递推关系式如下：
      αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1)

我们的动态规划从时刻1开始，到时刻T结束，由于αT(i)表示在时刻T观测序列为o1,o2,...oT，并且时刻T隐藏状态qi的概率，我们只要将所有隐藏状态对应的概率相加，即∑i=1NαT(i)就得到了在时刻T观测序列为o1,o2,...oT的概率。

　 下面总结下前向算法。

　　输入：HMM模型λ=(A,B,Π)，观测序列O=(o1,o2,...oT)
　　输出：观测序列概率P(O|λ)
　　1) 计算时刻1的各个隐藏状态前向概率：
      α1(i)=πibi(o1),i=1,2,...N
　　2) 递推时刻2,3,...T时刻的前向概率：
      αt+1(i)=[∑j=1Nαt(j)aji]bi(ot+1),i=1,2,...N
　　3) 计算最终结果：
      P(O|λ)=∑i=1NαT(i)
　　从递推公式可以看出，我们的算法时间复杂度是O(TN2)，比暴力解法的时间复杂度O(TNT)少了几个数量级。

c.鲍姆-韦尔奇算法原理
d.维特比算法原理