1. 贝叶斯理论

   贝叶斯理论是概率框架下实施决策的基本方法，贝叶斯定理实际上就是实际上就是计算
"条件概率"的公式。所谓"条件概率"（Conditional probability），就是指在事件B
发生的情况下，事件A发生的概率，用P(A|B)来表示。

    条件概率公式：
    P(AB) = P(A|B)P(B)
    P(AB) = P(B|A)P(A)
    因此：P(A|B)P(B) = P(B|A)P(A)
    即：P(A|B) = P(B|A)P(A) / P(B)
    这是条件概率的计算公式

   假定样本空间S，是两个事件A与A'的和。在样本空间中，存在着事件B。
   P(B) = P(BA) + P(BA1)
   由于P(AB) = P(B|A)P(A)
   所以，P(B) = P(B|A)P(A) + P(B|A1)P(A1)
   这就是全概率公式，如果A和A1构成样本空间的一个划分，那么事件B的概率，
   就等于A和A1的概率分别乘以B对这两个事件的条件概率之和。

   将该公式代入条件概率公式，就得到了条件概率公式的另一种写法：

   P(A|B) = P(B|A)P(A) / (P(B|A)P(A) + P(B|A1)P(A1))

   对于条件概率公式，把P(A)称为"先验概率"（Prior probability），即在B事件
   发生之前，我们对A事件概率的一个判断。P(A|B)称为"后验概率"（Posterior probability），
   即在B事件发生之后，我们对A事件概率的重新评估。P(B|A)/P(B)称为"可能性函数"
   （Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

   所以，条件概率可以理解成下面的式子：

   后验概率 = 先验概率 * 调整因子

   我们先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。
   如果"可能性函数"P(B|A)/P(B)>1，意味着"先验概率"被增强，事件A的发生的可能性变大；如果"可能性函数"=1，意味着B事件无助
   于判断事件A的可能性；如果"可能性函数"<1，意味着"先验概率"被削弱，事件A的
   可能性变小。

2. 朴素贝叶斯模型

   假如我们的分类模型样本是：即我们有m个样本，每个样本有n个特征，特征输出有K个类别，定义为C1,C2,...,CK。
   从样本我们可以学习得到朴素贝叶斯的先验分布P(Y=Ck)(k=1,2,...K)P(Y=Ck)(k=1,2,...K),接着学习到条件概率分布
   P(X=x|Y=Ck)=P(X1=x1,X2=x2,...Xn=xn|Y=Ck),然后我们就可以用贝叶斯公式得到X和Y的联合分布
   P(X,Y)了。联合分布P(X,Y)定义为：

      P(X,Y=Ck)=P(Y=Ck)P(X=x|Y=Ck)=P(Y=Ck)P(X1=x1,X2=x2,...Xn=xn|Y=Ck)

   其中，P(Y=Ck)P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)P(Y=Ck)就是类别CkCk在训练集里面出现的频数。
   但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。

   朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:
   P(X1=x1,X2=x2,...Xn=xn|Y=Ck)=P(X1=x1|Y=Ck)P(X2=x2|Y=Ck)...P(Xn=xn|Y=Ck)
   这一假设使得朴素贝叶斯变得简单，但是有时也会牺牲一定的分类准确率。

   





c.朴素贝叶斯的推断过程
d.朴素贝叶斯的参数估计
