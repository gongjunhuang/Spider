1.K近邻算法过程
  K近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该
实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分为这个类。

输入：训练数据集
    T = {(x1,y1), (x2,y2)...(xn,yn)}
其中，xi是实例的特征向量，yi是实例的类别
输出：实例x所属的类别y

（1）根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的领域
记做Nk(x)
（2）在Nk(x)中根据分类决策规则决定x的类别y



2.K近邻模型三要素
  KNN算法我们主要要考虑三个重要的要素，对于固定的训练集，只要这三点确定了，算法的
预测方式也就决定了。这三个最终的要素是k值的选取，距离度量的方式和分类决策规则。

  对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择
和距离的度量方式。

 对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通
 过交叉验证（cross_validation）选择一个合适的k值。

  选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入
实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，
换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
  选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，
但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器
作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
  一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预
测它属于在训练实例中最多的类，模型过于简单。

  对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离，即对于两个n维
向量x和y，两者的欧式距离定义为：
    D(x,y)=(x1−y1)2+(x2−y2)2+...+(xn−yn)2 = ∑i(xi−yi)2

也可以用其他距离进行度量，比如曼哈顿距离：
    D(x,y)=|x1−y1|+|x2−y2|+...+|xn−yn|=∑i|xi−yi|

其他还有闵可夫斯基距离等。

3.K近邻构造kd树

  KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，
建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，KD树中的K代表样本特征的维数。

  KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用方差最大的第k维特征
nknk来作为根节点。对于这个特征，我们选择特征nk的取值的中位数nkv对应的
样本作为划分点，对于所有第k维特征的取值小于nkv的样本，我们划入左子树，对于
第k维特征的取值大于等于nkv的样本，我们划入右子树，对于左子树和右子树，我们
采用和刚才同样的办法来找方差最大的特征来做根节点，递归的生成KD树。

  比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：

　　1）找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上
方差更大，用第1维特征建树。

　　2）确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即
中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过
（7,2）并垂直于：划分点维度的直线x=7；

　　3）确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x<=7的部分
为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节
点={(9,6)，(8,1)}。

　　4）用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)
，(8,1)}。最终得到KD树。

4.K近邻搜索kd树

  对于一个目标点，首先在KD树里面找到包含目标点的叶子节点。以目标点为圆心，
以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个
超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超
球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。
如果不相交那就简单了，直接返回父节点的父节点，在另一个子树继续搜索最近邻。
当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。

算法：
输入：已构造的kd树；目标点x
输出：x的最近邻
（1）在kd树中找出包含目标点x的叶节点：从根节点出发，递归地向下访问kd树，若
目标节点当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，
直到子节点是叶节点为止。
（2）以此叶节点为“当前最近点”
（3）递归地向上回退，在每个节点进行以下操作：
a. 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为当前最近点
b. 当前最近点一定存在于该节点一个子节点对应的区域，检查该子节点的父节点的另一子节点
对应的区域是否有更近的点，具体地，检查另一子节点对应的区域是否已目标点为球心、以目标点
与当前最近点之间的距离为半径的超球体相交。如果相交，则可能在另一个子节点对应的区域内存在距离
目标点更近的点，移动到另一个子节点，递归地进行最近邻搜索；如果不想交，回退。
c. 当回退到根节点，搜索结束。最后的当前最近点即为x的最近邻点。
