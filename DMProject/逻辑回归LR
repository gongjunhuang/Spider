a.线性回归和逻辑回归的联系

线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。此时Y是连续的，所以是回归模型。

在Y是离散的情况下，对于这个Y再做一次函数转换，变为g(Y)，令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。二元逻辑回归就是引入了一个sigmoid函数，将g(Y)映射在(-1, 1)之间。

b.二元逻辑回归的模型

Sigmoid函数的形式如下：
    g(z) = 1 / (1 + e^(-z))
当z趋于正无穷的时候，g(z)趋于1；而当z趋于负无穷的时候， g(z)趋于0，对于分类模型非常适用。
另外，它的导数性质为：g′(z)=g(z)(1−g(z))

如果令z=xθ， 则二元逻辑回归的一般模型为：
    hθ(x) = 1 / (1 + e^(-xθ))
    其中x为样本输入，hθ(x)为模型输入，可以理解为某一分类的概率大小。而θ为分类模型的要求出的模型参数。对于模型输出hθ(x)，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果hθ(x)>0.5 ，即xθ>0, 则y为1。如果hθ(x)<0.5，即xθ<0, 则y为0。y=0.5是临界情况，此时xθ=0为， 从逻辑回归模型本身无法确定分类。

hθ(x)的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。

二元回归模型的矩阵形式为：
    hθ(X) = 1 / (1 + e^(-Xθ))

其中hθ(X)为模型输出，为mx1的维度。X为样本特征矩阵，为mxn的维度。θ为分类的模型系数，为nx1的向量。


c.二元逻辑回归的损失函数


按照二元逻辑回归的定义，样本输出为0和1两类，则有：
    P(y=1|x,θ)=hθ(x) 
　　 P(y=0|x,θ)=1−hθ(x)
合并两式，则有P(y|x,θ)=hθ(x)y(1−hθ(x))^(1−y)
其中y的取值只能是0或1

得到了y的概率分布函数表达式，就可以用似然函数最大化来求解我们需要的模型系数θ。
为了方便求解，用对数似然函数最大化，对数似然函数取反即为损失函数J(θ)。其中：似然函数的代数表达式为：
    L(θ)=∏(hθ(x(i)))^y(i)(1−hθ(x^(i)))^(1−y(i))
其中m为样本的个数

对似然函数对数化取反的表达式，即损失函数表达式为：
J(θ)=−lnL(θ)=−∑(y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i))))


e.多元逻辑回归

从二元回归到多元逻辑回归，可以认为某种类型为正值，其余为0值，这种方法为最常用的one-vs-reset，简称OvR.另一种多元逻辑回归的方法是Many-vs-Many(MvM)，它会选择一部分类别的样本和另一部分类别的样本来做逻辑回归二分类。最常用的是One-Vs-One（OvO）。OvO是MvM的特例。每次我们选择两类样本来做二元逻辑回归。

根据二元逻辑回归，我们可以得到：
lnP(y=1|x,θ)/P(y=0|x,θ)=xθ

　我们假设是K元分类模型,即样本输出y的取值为1，2，。。。，K。
根据二元逻辑回归的经验，我们有：
lnP(y=1|x,θ)/P(y=k|x,θ)=xθ
lnP(y=2|x,θ)/P(y=k|x,θ)=xθ
...
lnP(y=k-1|x,θ)/P(y=k|x,θ)=xθ
共k-1个方程。

加上概率之和为1的公式∑P(y=i|x,θ)=1，所以共有k个方程。
解出k元一次方程组，得到k元逻辑回归的概率为：
    P(y=k|x,θ)=e^(xθk)/(1+∑e^(xθt)) 　 k = 1,2,...K-1


f.最大熵模型的定义

最大熵模型假设分类模型是一个条件概率分布P(Y|X)P(Y|X),X为特征，Y为输出。

给定一个训练集(x(1),y(1)),(x(2),y(2)),...，(x(m),y(m)),其中x为n维特征向量，y为类别输出。我们的目标就是用最大熵模型选择一个最好的分类类型。

在给定训练集的情况下，我们可以得到总体联合分布P(X,Y)的经验分布P¯(X,Y),和边缘分布P(X)的经验分布P¯(X)。P¯(X,Y)即为训练集中X,Y同时出现的次数除以样本总数m，P¯(X)即为训练集中X出现的次数除以样本总数m。

用特征函数f(x,y)描述输入x和输出y之间的关系。定义为：
f(x,y)=1  x与y满足某个关系否则 0
可以认为只要出现在训练集中出现的(x(i),y(i)),其f(x(i),y(i))=1. 同一个训练样本可以有多个约束特征函数。

特征函数f(x,y)关于经验分布P¯(X,Y)的期望值，用)EP¯(f)表示为:
    EP¯(f)=∑P¯(x,y)f(x,y)

特征函数f(x,y)关于条件分布P(Y|X)和经验分布P¯(X)的期望值，用EP(f)表示为:
    EP(f)=∑P¯(x)P(y|x)f(x,y)

如果模型可以从训练集中学习，我们就可以假设这两个期望相等。即：
    EP¯(f) = EP(f)

上式就是最大熵模型学习的约束条件，假如我们有M个特征函数fi(x,y)(i=1,2...,M)就有M个约束条件。可以理解为我们如果训练集里有m个样本，就有和这m个样本对应的M个约束条件。

假设满足所有约束条件的模型集合为：
    EP¯(fi)=EP(fi)(i=1,2,...M)

定义在条件概率分布P(Y|X)上的条件熵为：
    H(P)=−∑P¯(x)P(y|x)logP(y|x)

我们的目标是得到使H(P)最大的时候对应的P(y|x),这里可以对H(P)加了个负号求极小值，这样做的目的是为了使−H(P)为凸函数，方便使用凸优化的方法来求极值。

