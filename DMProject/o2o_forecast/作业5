a.决策树ID3算法

   信息增益（information gain）
   熵（entrofy）：表示随机变量不确定性的度量。熵度量了事物的不确定性，越不确定的事物，它的熵就越大。
   具体的，随机变量X的熵的表达式如下：
         H(X) = -∑pilogpi
   其中n代表X的n种不同的离散取值。而pipi代表了X取值为i的概率，log为以2或者e为底的对数。

   因此，两个变量X和Y的联合熵表达式：
         H(X, Y) = -∑p(xi, yi)logp(xi, yi)

   条件熵类似于条件概率,它度量了我们的X在知道Y以后剩下的不确定性。表达式如下：
         H(X|Y) = -∑p(xi, yi)logp(xi|yi)

   H(X)-H(X|Y)就叫做信息增益，它度量了X在知道Y以后不确定性减少程度。
   ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。

   输入的是m个样本，样本输出集合为D，每个样本有n个离散特征，特征集合即为A，输出为决策树T。

　　算法的过程为：

　　　　1)初始化信息增益的阈值ϵ
　　　　2）判断样本是否为同一类输出Di，如果是则返回单节点树T。标记类别为Di
　　　　3) 判断特征是否为空，如果是则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。

　　　　4）计算A中的各个特征（一共n个）对输出D的信息增益，选择信息增益最大的特征Ag
　　　　5) 如果Ag的信息增益小于阈值ϵ，则返回单节点树T，标记类别为样本中输出类别D实例数最多的类别。

　　　　6）否则，按特征Ag的不同取值Agi将对应的样本输出D分成不同的类别Di。每个类别产生一个子节点。对应特征值为Agi。返回增加了节点的数T。

　　　　7）对于所有的子节点，令D=Di,A=A−{Ag}递归调用2-6步，得到子树Ti并返回。

   ID3算法虽然提出了新思路，但是还是有很多值得改进的地方。　　

　　　　a)ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。

　　　　b)ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。如果校正这个问题呢？

　　　　c) ID3算法对于缺失值的情况没有做考虑

　　　　d) 没有考虑过拟合的问题

b.决策树C4.5算法

   C4.5算法主要解决了ID3算法的几个不足。

   1. 不能处理连续特征， C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为
   a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示Ti表示为：
      Ti = (ai + ai+1) / 2
   对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。
   比如取到的增益最大的点为at,则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，
   如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。

   2. 信息增益作为标准容易偏向于取值较多的特征的问题。我们引入一个信息增益比的变量gR(D, A)，它是信息增益和特征熵的比值。
      gR(D, A) = g(D, A) / H(D)

   特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。


   3. 对于第三个缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，
   对于在该属性上缺失特征的样本的处理。
   对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，
   一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的
   信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

   对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。
   比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时
   划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

   4. 对于过拟合问题，C4.5引入了正则化系数进行初步的剪枝。

   C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间。

　　1. 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。后面在下篇讲CART树的时候我们会专门讲决策树的减枝思路，主要采用的是后剪枝加上交叉验证选择最合适的决策树。

　　2. C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。

　　3. C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。

　　4. C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。

c.决策树CART算法

   CART树（classification and regression tree），分类与回归树，是在给定输入随机变量X条件下
   输出随机变量Y的条件概率分布的学习方法。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。

   具体的，在分类问题中，假设有K个类别，第k个类别的概率为pk, 则基尼系数的表达式为：
      Gini = ∑pk(1-pk) = 1 - ∑pk^2
   
   如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：
      Gini(p) = 2p(1-p)

   对于个给定的样本D,假设有K个类别, 第k个类别的数量为Ck,则样本D的基尼系数表达式为：
      Gini(D) = 1 - ∑(Ck/D)^2

   CART分类连续值的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为
   a1,a2,...,am,则CART取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示Ti表示为：
      Ti = (ai + ai+1) / 2
   对于这m-1个点，分别计算以该点作为二元分类点时的Gini系数。选择Gini系数最小的点作为该连续特征的二元离散分类点。

   对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。

   算法输入是训练集D，基尼系数的阈值，样本个数阈值。

　 输出是决策树T。

　　我们的算法从根节点开始，用训练集递归的建立CART树。

　　1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。

　　2) 计算样本集D的基尼系数，如果基尼系数大于阈值，则返回决策树子树，当前节点停止递归。

　　3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数。缺失值的处理方法和C4.5算法里描述的相同。

　　4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.

　　5) 对左右的子节点递归的调用1-4步，生成决策树。

　　对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。



d.以上三个算法的区别和联系（缺失值处理，离散变量和连续变量如何处理，是否适合做分类或是回归）

   1. 缺失值处理：
   ID3算法对于缺失值的情况没有做考虑；C4.5中，对于缺失值处理的问题，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，
   对于在该属性上缺失特征的样本的处理。
   对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，
   一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的
   信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

   对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。
   比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时
   划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

   对于CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。


   2. ID3不能处理连续特征； C4.5的思路是将连续的特征离散化；对于CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。

   3. 决策树可以用来做分类和回归。CART树可以分为CART分类树和CART回归树。


e.C4.5算法和CART树算法的剪枝

   剪枝一般分两种方法：预剪枝和后剪枝。

    预剪枝方法中通过提前停止树的构造（比如决定在某个节点不再分裂或划分训练元组的子集）而对树剪枝。一旦停止，这个节点就变成树叶，该树叶可能取它持有的子集最频繁的类作为自己的类。先剪枝有很多方法，比如（1）当决策树达到一定的高度就停止决策树的生长；（2）到达此节点的实例具有相同的特征向量，而不必一定属于同一类，也可以停止生长（3）到达此节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数据量比较小的特殊情况（4）计算每次扩展对系统性能的增益，如果小于某个阈值就可以让它停止生长。先剪枝有个缺点就是视野效果问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步扩展又能满足要求。这样会过早停止决策树的生长。

    另一种更常用的方法是后剪枝，它由完全成长的树剪去子树而形成。通过删除节点的分枝并用树叶来替换它。树叶一般用子树中最频繁的类别来标记。

    C4.5采用悲观剪枝法，它使用训练集生成决策树又用它来进行剪枝，不需要独立的剪枝集。
    悲观剪枝法的基本思路是：设训练集生成的决策树是T，用T来分类训练集中的N的元组，设K为到达某个叶子节点的元组个数，其中分类错误地个数为J。由于树T是由训练集生成的，是适合训练集的，因此J/K不能可信地估计错误率。所以用(J+0.5)/K来表示。设S为T的子树，其叶节点个数为L(s)，∑K为到达此子树的叶节点的元组个数总和，∑J为此子树中被错误分类的元组个数之和。在分类新的元组时，则其错误分类个数为∑J + L(s)/2。

    CART树算法的剪枝：
    由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的返回能力。但是，有很多的剪枝方法，我们应该这么选择呢？CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。

    也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。

    首先我们看看剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：
        Cα(Tt)=C(Tt)+α|Tt|
   其中，αα为正则化参数，这和线性回归的正则化一样。C(Tt)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|Tt|是子树T的叶子节点的数量。

   当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数Cα(T)最小的唯一子树。

   CART树的剪枝算法。

　 输入是CART树建立算法得到的原始决策树T。

　　输出是最优决策子树Tα。

　　算法过程如下：

　　1）初始化αmin=∞， 最优子树集合ω={T}。

　　2）从叶子节点开始自下而上计算各内部节点t的训练误差损失函数Cα(Tt)（回归树为均方差，分类树为基尼系数）, 叶子节点数|Tt|，以及正则化阈值α=min{C(T)−C(Tt)|Tt|−1,αmin}, 更新αmin=α
　　3) 得到所有节点的α值的集合M。

　　4）从M中选择最大的值αk，自上而下的访问子树t的内部节点，如果C(T)−C(Tt)|Tt|−1≤αk时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到αkαk对应的最优子树Tk
　　5）最优子树集合ω=ω∪Tk， M=M−{αk}。

　　6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω.

　　7) 采用交叉验证在ω选择最优子树Tα。
