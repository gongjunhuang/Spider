决策树：从根节点一步一步走到叶子节点

根节点-----非叶子节点-----叶子节点

每个节点相当于在数据中切分一次，所以并不是节点越多越好

训练与测试：

训练阶段：从给定的训练集中构造出来一棵树，从根节点开始选择特征，如何进行特征切分

测试阶段：根据构造出来的树模型从上到下走一遍

一旦构造好了决策树，那么分类或者预测任务很简单了，主要问题在于构造一棵树

如何切分特征：

通过一种衡量标准，来计算通过不同特征进行分支选择后的分类情况，找出来最好的那个当根节点

熵：随机变量不确定性的度量

信息增益：表示特征X使得类Y的不确定性减少的程度。

CART：Classification and Regression Trees,分类回归树
使用GINI系数当作衡量标准      1 - p^2

剪枝策略：决策树过拟合风险很大，理论上可以完全分得开数据

策略：预剪枝（边建立决策树边剪枝），使用较多  。限制深度，叶子节点个数，叶子节点样本数，信息增益量等

后剪枝（当建立完决策树后来进行剪枝），通过一定标准衡量

```
class decision_node:
    def __init__(self, col=-1, value=None, results=None, tb=None, fb=None):
        self.col = col
        self.value = value
        self.results = results
        self.tb = tb
        self.fb = fb

    # 在某一列上堆数据集合进行拆分，能够处理数值型数据或者名词性数据
    def divideset(rows, column, value):
        # 定义一个函数，令其告诉我们数据行属于第一个（返回值为true）还是第二组（返回值为false）
        split_function = None
        if isinstance(value, int) or isinstance(value, float):
            split_function = lambda row : row[column]>=value
        else:
            split_function = lambda row:row[column] == value

        # 将数据集拆分成两个集合，并返回
        set1 = [row for row in rows if split_function(row)]
        set2 = [row for row in rows if not split_function(row)]
        return (set1, set2)

    def uniquecounts(rows):
        results = {}
        for row in rows:
            r = row[len(row) - 1]
            if r not in results : results[r]=0
            results[r] += 1
        return results

    #基尼不纯度
    def giniimpurity(rows):
        total = len(rows)
        counts = uniquecounts(rows)
        imp = 0
        for k1 in counts:
            p1 = float(counts[k1]) / total
            for k2 in counts:
                if k1 == k2:continue
                p2 = float(counts[k2])/total
                imp += p1*p2
        return imp

    #熵
    def entropy(rows):
        from math import log
        log2 = lambda x:log(x)/log(2)
        results = uniquecounts(rows)
        # 开始计算熵值
        ent = 0.0
        for r in results.keys():
            p = float(results[r]) / len(rows)
            ent = ent - p * log2(p)
        return ent

    def build_tree(rows, score=entropy):
        if len(rows) == 0:
            return decisionnode()

        current_score = scoref(rows)

        #定义一些最佳变量以记录最佳拆分条件
        best_gain = 0.0
        best_criteria = None
        best_sets = None

        column_count = len(rows[0]) - 1
        for col in range(0, column_count):
            #在当前列生成一个由不同值构成的序列
            column_values = {}
            for row in rows:
                column_values[row[col]] = 1

            # 根据这一列中每个值尝试对数据集进行拆分
            for value in column_values.keys():
                (set1, set2) = divideset(rows, col, value)

                # 信息增益
                p = float(len(set1)) / len(rows)
                gain = current_score - p * scoref(set1) - (1-p) * scoref(set2)
                if gain > best_gain and len(set1) > 0 and len(set2) > 0:
                    best_gain = gain
                    best_criteria = (col, value)
                    best_sets = (set1, set2)

            if best_gain >  0:
                trueBranch = build_tree(best_sets[0])
                falseBranch = build_tree(best_sets[1])
                return decision_node
```












利用sklearn构造决策树模型

```
# datasets包括内置的数据集 california_housing房价的数据集
from sklearn.datasets.california_housing import fetch_california_housing
import pandas as pd

housing = fetch_california_housing()
# print(housing.DESCR)
# print(housing.data)
# print(housing.data.shape) #(20640, 8)
# print(housing.target)
# print(housing.feature_names)
# #['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']

from sklearn import tree

dtr = tree.DecisionTreeRegressor(max_depth=2)  # DecisionTreeRegressor 决策树 max_depth 树的最大深度
# 指定某几列
dtr.fit(housing.data[:, [6, 7]], housing.target)  # latitude longitude  纬度经度 传入：X y
# print(dtr)
'''''
DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,
           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, presort=False, random_state=None,
           splitter='best')
'''

'''''决策树模型可视化'''

dot_data = \
    tree.export_graphviz(
        dtr,  # 构造矩阵名字
        out_file="tree.dot",
        feature_names=housing.feature_names[6:8],  # 特征名字
        filled=True,
        impurity=False,
        rounded=True
    )

import pydotplus
from IPython.display import Image

graph = pydotplus.graph_from_dot_file("tree.dot")
graph.get_nodes()[7].set_fillcolor("#FFF2DD")
Image(graph.create_png())
graph.write_png("dtr_white_background.png")  # 保存为本地图片

from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(housing.data[0:1000], housing.target[0:1000], test_size=0.1,
                                                    random_state=42)  # random_state = 42 值随意，保证每次随机完结果一样
dtr = tree.DecisionTreeRegressor(random_state=42)
dtr.fit(X_train, y_train)
print(dtr.score(X_test, y_test))

'''''随机森林'''
from sklearn.grid_search import GridSearchCV  # GridSearchCV 自动设置参数组合
from sklearn.ensemble import RandomForestRegressor
#参数候选项写成字典格式

树模型参数:

1.criterion gini or entropy

2.splitter best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）

3.max_features None（所有），log2，sqrt，N 特征小于50的时候一般使用所有的

4.max_depth 数据少或者特征少的时候可以不管这个值，如果模型样本量多，特征也多的情况下，可以尝试限制下

5.min_samples_split 如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

6.min_samples_leaf 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝，如果样本量不大，不需要管这个值，大些如10W可是尝试下5

7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

8.max_leaf_nodes 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制具体的值可以通过交叉验证得到。

9.class_weight 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。

10.min_impurity_split 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值则该节点不再生成子节点。即为叶子节点 。

11.n_estimators:要建立树的个数



tree_param_grid = {"min_samples_split": list((3, 6, 9)), "n_estimators": list((10, 50, 100))}
grid = GridSearchCV(RandomForestRegressor(), param_grid=tree_param_grid, cv=5)  # cv交叉验证(切分的是测试集)
grid.fit(X_train, y_train)
print(grid.grid_scores_)
print(grid.best_params_)
print(grid.best_score_)

rfr = RandomForestRegressor(min_samples_split=3, n_estimators=100, random_state=42)
rfr.fit(X_train, y_train)
rfr.score(X_test, y_test)

pd.Series(rfr.feature_importances_, index=housing.feature_names).sort_values(ascending=False)
```
