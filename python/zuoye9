二项分布
二项试验
满足以下条件的试验成为二项试验：

试验由一系列相同的n个试验组成；
每次试验有两种可能的结果，成功或者失败；
每次试验成功的概率是相同的，用p来表示；
试验是相互独立的。
设x为n次试验中的成功的次数，由于随机变量的个数是有限的，所以x是一个离散型随机变量。x的概率分布成为二项分布

>>> import numpy as np
>>> list_a = np.random.binomial(n, p, size=None)

n为试验次数
p为出现正例（试验成功）的概率
size为取样次数，即重复进行此二项试验的次数
函数返回值为每次取样出现的成功试验的次数

>>> import numpy as np
>>> import matplotlib.pyplot as plt
>>> list_a = np.random.binomial(n=10,p=0.2,size=1000)
#取样1000次，每次进行十组试验，单组试验成功概率为0.2，list_a为每组试验中成功的组数
>>> plt.hist(list_a,bins=8,color='g',alpha=0.4,edgecolor='b')
(array([ 157.,  240.,  236.,  208.,   86.,   57.,   13.,    3.]), array([ 0.   ,  1.125,  2.25 ,  3.375,  4.5  ,  5.625,  6.75 ,  7.875,  9.   ]), <a list of 8 Patch objects>)
>>> plt.show()

泊松分布
泊松试验
满足以下条件的试验成为泊松试验：

在任意两个相等长度的区间上，事件发生的概率相等；
事件在某一区间上是否发生与事件在其他区间上是否发生所独立的。


>>> list_b = np.random.poisson(8,1000)#试验重复1000次
>>> plt.hist(list_b,bins=8,color='g',alpha=0.4,edgecolor='b')
(array([  14.,   71.,  201.,  269.,  236.,  132.,   60.,   17.]), array([  1.,   3.,   5.,   7.,   9.,  11.,  13.,  15.,  17.]), <a list of 8 Patch objects>)
>>> plt.show()




正态分布
当二项分布的样本数量足够大时，其分布曲线会变成对称的钟形，我们将这种分布形态成为正态分布

Python实现及图像
list_d = np.random(loc,scale,size=None)

>>> list_d = np.random.normal(0,1,1000)
>>> plt.hist(list_d,bins=8,color='g',alpha=0.4,edgecolor='b')
(array([  11.,   53.,  158.,  321.,  264.,  145.,   39.,    9.]), array([-3.34109196, -2.50103319, -1.66097443, -0.82091566,  0.0191431 ,
        0.85920186,  1.69926063,  2.53931939,  3.37937815]), <a list of 8 Patch objects>)
>>> plt.show()


指数分布
指数分布与泊松分布类似，泊松分布描述了每一个区间内事件发生的次数，而指数分布描述了事件发生的事件间隔长度。
设一个某站台平均每小时会经过8辆公共汽车，求两辆公共汽车间隔时间不超过x小时的概率：

Python实现及图像

>>> list_e = np.random.exponential(0.125,1000)
>>> plt.hist(list_e,bins=8,color='g',edgecolor='b',alpha=0.4)
(array([ 552.,  250.,  121.,   49.,   15.,    6.,    4.,    3.]), array([  1.38250181e-04,   1.06106465e-01,   2.12074680e-01,
         3.18042896e-01,   4.24011111e-01,   5.29979326e-01,
         6.35947541e-01,   7.41915756e-01,   8.47883971e-01]), <a list of 8 Patch objects>)
>>> plt.show()






中心极限定理表示大量相互独立的随机变量，其均值的分布的极限是正态分布。


点估计和区间估计属于统计学的内容，是针对参数估计而言的。（参数估计：对一个给定的分布，估计分布中的某个参数。）

点估计顾名思义就是用一个具体的值（统计量）来得到待估参数的值。区间估计就是用一个区间（统计量表示的区间）来得到以1-alpha概率包含待估参数的区间。




首先说点估计。点估计就是用一个数据（data）的函数（通常称为估计统计量，estimator）来给出一个未知参数的估计值。即使是固定的参数真值（虽然我们不知道这个值），由于数据的随机性，不同的数据代入这个函数往往会得出不同的估计值（estimation ）。所以我们往往在点估计的基础上包裹上一个邻域，即得到一个区间估计。

那么点估计周围的这个邻域的大小是怎么确定的呢？一个最直接的答案就是：确定一个百分比，p%，使得给定任意数据集，参数的估计值（estimation）落在这个邻域内的概率为p%。那么，确定邻域大小的问题就变成了确定参数估计量（estimator）的分布的问题了。首先，如果我们假设数据服从正态分布。那么可以证明，统计量作为随机变量的函数，往往会服从从正态分布中推导出来的一系列分布（如t分布，chi-square分布和F分布），那么通过统计量（estimator）的分布，我们可以很轻松的得到所求邻域的大小。接下来的问题就是，在日常生活中，数据并不一定服从正态分布的。如果数据不是正态分布的，那么估计统计量（estimator）很可能也不服从t分布，chi-square分布和F分布这些我们已知的分布。如果我们不知道统计量的分布，就无法确定应该给这个点估计包裹一个多大的邻域。于是我们退而求其次，由于在满足一定正则条件的情况下，很多数据的分布都会在数据量趋近于无穷的情况下趋近于正态分布。如果数据的分布恰好落在这个范围内，那么我们说，在数据量趋近于无穷的前提下，我们仍然相信统计量服从t分布，chi-square分布和F分布这些我们已知的分布。并以此为基础得到区间估计。而中心极限定理（CLT）就是用来保证数据分布的极限为正态分布的定理。估计量一般可以表示成样本均值的函数（e.g. OLS，GMM） 所以知道了样本均值的极限（正态）分布也就知道了这些估计量的极限分布。于是我们就可以计算区间估计中的区间了最后，如果正则条件不满足，CLT无法适用。数据分布即使在数据量趋于无穷的情况下仍然不是正态分布，这时候，采用传统方法得到区间估计的办法就行不通了。需要采用更加先进的方法（比如bootstrapping寻找区间估计；比如彻底抛弃parametric model转用semi- non-parametric model等等）。

