分类器评估指标
对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为：
真正例（True Positive，TP）：真实类别为正例，预测类别为正例。
假正例（False Positive，FP）：真实类别为负例，预测类别为正例。
假负例（False Negative，FN）：真实类别为正例，预测类别为负例。
真负例（True Negative，TN）：真实类别为负例，预测类别为负例。

ROC AUC(Area Under ROC Curve): ROC 曲线和 AUC 常被用来评价一个二值分类器的优劣。若一个学习器的ROC曲线被另一个包住，后者的性能能优于前者；若交叉，判断ROC曲线下的面积，即AUC.

precision   P = TP/(TP+FP)

recall    R = TP / (TP+FN)

F1 = 2PR/(P+R)

平均准确率(Average Per-class Accuracy):
  为了应对每个类别下样本的个数不一样的情况，计算每个类别下的准确率，然后再计算它们的平均值。

对数损失函数(Log-loss):
  在分类输出中，若输出不再是0-1，而是实数值，即属于每个类别的概率，那么可以使用Log-loss对分类结果进行评价。这个输出概率表示该记录所属的其对应的类别的置信度。比如如果样本本属于类别0，但是分类器则输出其属于类别1的概率为0.51，那么这种情况认为分类器出错了。该概率接近了分类器的分类的边界概率0.5。Log-loss是一个软的分类准确率度量方法，使用概率来表示其所属的类别的置信度。



K近邻

1.K近邻算法过程
  K近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该
实例最邻近的k个实例，这k个实例的多数属于某个类，就把该实例分为这个类。

输入：训练数据集
    T = {(x1,y1), (x2,y2)...(xn,yn)}
其中，xi是实例的特征向量，yi是实例的类别
输出：实例x所属的类别y

（1）根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的领域
记做Nk(x)
（2）在Nk(x)中根据分类决策规则决定x的类别y



2.K近邻模型三要素
  KNN算法我们主要要考虑三个重要的要素，对于固定的训练集，只要这三点确定了，算法的
预测方式也就决定了。这三个最终的要素是k值的选取，距离度量的方式和分类决策规则。

  对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择
和距离的度量方式。

 对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通
 过交叉验证（cross_validation）选择一个合适的k值。

  选择较小的k值，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入
实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，
换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
  选择较大的k值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少泛化误差，
但缺点是训练误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器
作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
  一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预
测它属于在训练实例中最多的类，模型过于简单。

  对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离，即对于两个n维
向量x和y，两者的欧式距离定义为：
    D(x,y)=(x1−y1)2+(x2−y2)2+...+(xn−yn)2 = ∑i(xi−yi)2

也可以用其他距离进行度量，比如曼哈顿距离：
    D(x,y)=|x1−y1|+|x2−y2|+...+|xn−yn|=∑i|xi−yi|

其他还有闵可夫斯基距离等。

3.K近邻构造kd树

  KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，
建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，KD树中的K代表样本特征的维数。

  KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用方差最大的第k维特征
nknk来作为根节点。对于这个特征，我们选择特征nk的取值的中位数nkv对应的
样本作为划分点，对于所有第k维特征的取值小于nkv的样本，我们划入左子树，对于
第k维特征的取值大于等于nkv的样本，我们划入右子树，对于左子树和右子树，我们
采用和刚才同样的办法来找方差最大的特征来做根节点，递归的生成KD树。

  比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：

　　1）找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上
方差更大，用第1维特征建树。

　　2）确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即
中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过
（7,2）并垂直于：划分点维度的直线x=7；

　　3）确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x<=7的部分
为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节
点={(9,6)，(8,1)}。

　　4）用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)
，(8,1)}。最终得到KD树。

4.K近邻搜索kd树

  对于一个目标点，首先在KD树里面找到包含目标点的叶子节点。以目标点为圆心，
以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个
超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超
球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。
如果不相交那就简单了，直接返回父节点的父节点，在另一个子树继续搜索最近邻。
当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。

算法：
输入：已构造的kd树；目标点x
输出：x的最近邻
（1）在kd树中找出包含目标点x的叶节点：从根节点出发，递归地向下访问kd树，若
目标节点当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，
直到子节点是叶节点为止。
（2）以此叶节点为“当前最近点”
（3）递归地向上回退，在每个节点进行以下操作：
a. 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为当前最近点
b. 当前最近点一定存在于该节点一个子节点对应的区域，检查该子节点的父节点的另一子节点
对应的区域是否有更近的点，具体地，检查另一子节点对应的区域是否已目标点为球心、以目标点
与当前最近点之间的距离为半径的超球体相交。如果相交，则可能在另一个子节点对应的区域内存在距离
目标点更近的点，移动到另一个子节点，递归地进行最近邻搜索；如果不想交，回退。
c. 当回退到根节点，搜索结束。最后的当前最近点即为x的最近邻点。




Logistic

a.线性回归和逻辑回归的联系

线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足Y=Xθ。此时Y是连续的，所以是回归模型。

在Y是离散的情况下，对于这个Y再做一次函数转换，变为g(Y)，令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。二元逻辑回归就是引入了一个sigmoid函数，将g(Y)映射在(-1, 1)之间。

b.二元逻辑回归的模型

Sigmoid函数的形式如下：
    g(z) = 1 / (1 + e^(-z))
当z趋于正无穷的时候，g(z)趋于1；而当z趋于负无穷的时候， g(z)趋于0，对于分类模型非常适用。
另外，它的导数性质为：g′(z)=g(z)(1−g(z))

如果令z=xθ， 则二元逻辑回归的一般模型为：
    hθ(x) = 1 / (1 + e^(-xθ))
    其中x为样本输入，hθ(x)为模型输入，可以理解为某一分类的概率大小。而θ为分类模型的要求出的模型参数。对于模型输出hθ(x)，我们让它和我们的二元样本输出y（假设为0和1）有这样的对应关系，如果hθ(x)>0.5 ，即xθ>0, 则y为1。如果hθ(x)<0.5，即xθ<0, 则y为0。y=0.5是临界情况，此时xθ=0为， 从逻辑回归模型本身无法确定分类。

hθ(x)的值越小，而分类为0的的概率越高，反之，值越大的话分类为1的的概率越高。如果靠近临界点，则分类准确率会下降。

二元回归模型的矩阵形式为：
    hθ(X) = 1 / (1 + e^(-Xθ))

其中hθ(X)为模型输出，为mx1的维度。X为样本特征矩阵，为mxn的维度。θ为分类的模型系数，为nx1的向量。


c.二元逻辑回归的损失函数


按照二元逻辑回归的定义，样本输出为0和1两类，则有：
    P(y=1|x,θ)=hθ(x) 
　　 P(y=0|x,θ)=1−hθ(x)
合并两式，则有P(y|x,θ)=hθ(x)y(1−hθ(x))^(1−y)
其中y的取值只能是0或1

得到了y的概率分布函数表达式，就可以用似然函数最大化来求解我们需要的模型系数θ。
为了方便求解，用对数似然函数最大化，对数似然函数取反即为损失函数J(θ)。其中：似然函数的代数表达式为：
    L(θ)=∏(hθ(x(i)))^y(i)(1−hθ(x^(i)))^(1−y(i))
其中m为样本的个数

对似然函数对数化取反的表达式，即损失函数表达式为：
J(θ)=−lnL(θ)=−∑(y(i)log(hθ(x(i)))+(1−y(i))log(1−hθ(x(i))))


e.多元逻辑回归

从二元回归到多元逻辑回归，可以认为某种类型为正值，其余为0值，这种方法为最常用的one-vs-reset，简称OvR.另一种多元逻辑回归的方法是Many-vs-Many(MvM)，它会选择一部分类别的样本和另一部分类别的样本来做逻辑回归二分类。最常用的是One-Vs-One（OvO）。OvO是MvM的特例。每次我们选择两类样本来做二元逻辑回归。

根据二元逻辑回归，我们可以得到：
lnP(y=1|x,θ)/P(y=0|x,θ)=xθ

　我们假设是K元分类模型,即样本输出y的取值为1，2，。。。，K。
根据二元逻辑回归的经验，我们有：
lnP(y=1|x,θ)/P(y=k|x,θ)=xθ
lnP(y=2|x,θ)/P(y=k|x,θ)=xθ
...
lnP(y=k-1|x,θ)/P(y=k|x,θ)=xθ
共k-1个方程。

加上概率之和为1的公式∑P(y=i|x,θ)=1，所以共有k个方程。
解出k元一次方程组，得到k元逻辑回归的概率为：
    P(y=k|x,θ)=e^(xθk)/(1+∑e^(xθt)) 　 k = 1,2,...K-1


f.最大熵模型的定义

最大熵模型假设分类模型是一个条件概率分布P(Y|X)P(Y|X),X为特征，Y为输出。

给定一个训练集(x(1),y(1)),(x(2),y(2)),...，(x(m),y(m)),其中x为n维特征向量，y为类别输出。我们的目标就是用最大熵模型选择一个最好的分类类型。

在给定训练集的情况下，我们可以得到总体联合分布P(X,Y)的经验分布P¯(X,Y),和边缘分布P(X)的经验分布P¯(X)。P¯(X,Y)即为训练集中X,Y同时出现的次数除以样本总数m，P¯(X)即为训练集中X出现的次数除以样本总数m。

用特征函数f(x,y)描述输入x和输出y之间的关系。定义为：
f(x,y)=1  x与y满足某个关系否则 0
可以认为只要出现在训练集中出现的(x(i),y(i)),其f(x(i),y(i))=1. 同一个训练样本可以有多个约束特征函数。

特征函数f(x,y)关于经验分布P¯(X,Y)的期望值，用)EP¯(f)表示为:
    EP¯(f)=∑P¯(x,y)f(x,y)

特征函数f(x,y)关于条件分布P(Y|X)和经验分布P¯(X)的期望值，用EP(f)表示为:
    EP(f)=∑P¯(x)P(y|x)f(x,y)

如果模型可以从训练集中学习，我们就可以假设这两个期望相等。即：
    EP¯(f) = EP(f)

上式就是最大熵模型学习的约束条件，假如我们有M个特征函数fi(x,y)(i=1,2...,M)就有M个约束条件。可以理解为我们如果训练集里有m个样本，就有和这m个样本对应的M个约束条件。

假设满足所有约束条件的模型集合为：
    EP¯(fi)=EP(fi)(i=1,2,...M)

定义在条件概率分布P(Y|X)上的条件熵为：
    H(P)=−∑P¯(x)P(y|x)logP(y|x)

我们的目标是得到使H(P)最大的时候对应的P(y|x),这里可以对H(P)加了个负号求极小值，这样做的目的是为了使−H(P)为凸函数，方便使用凸优化的方法来求极值。

