特点：

支持三种分词模式：

精确模式，试图将句子最精确地切开，适合文本分析；

全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；

搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

支持繁体分词

支持自定义词典

MIT 授权协议

**1. 分词**

**jieba.cut** 方法接受三个输入参数: 需要分词的字符串；

**cut_all** 参数用来控制是否采用全模式；

**HMM **参数用来控制是否使用 HMM 模型

**jieba.cut_for_search **方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8

jieba.cut以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用

jieba.lcut 以及jieba.lcut_for_search 直接返回 list

**jieba.Tokenizer(dictionary=DEFAULT_DICT)** 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。


**载入词典**

开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
用法：** jieba.load_userdict(file_name**) # file_name 为文件类对象或自定义词典的路径

词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。
词频省略时使用自动计算的能保证分出该词的词频。


**调整词典**

使用** add_word(word, freq=None, tag=None)和** del_word(word) 可在程序中动态修改词典

使用 **suggest_freq(segment, tune=True) **可调节单个词语的词频，使其能（或不能）被分出来。

注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。


**基于 TF-IDF 算法的关键词抽取**

import jieba.analyse

**jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())**

**sentence** 为待提取的文本

**topK** 为返回几个 TF/IDF 权重最大的关键词，默认值为 20

**withWeight **为是否一并返回关键词权重值，默认值为 False

**allowPOS** 仅包括指定词性的词，默认值为空，即不筛选

jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件


**词性标注**

**jieba.posseg.POSTokenizer(tokenizer=None)** 新建自定义分词器，tokenizer参数可指定内部使用的jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。
标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。

**a 形容词       c连词      d副词      e叹词      f   方位词      i成语    m  量词     n名词    nr   人名      ns  地名   nt 机构团体      其他常用专有名词        p  介词    r  代词     t时间      u助词     v动词   vn动名词    w标点符号     un未知词语**


**并行分词**

原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升

基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows

用法：

**jieba.enable_parallel(4)**# 开启并行分词模式，参数为并行进程数

**jieba.disable_parallel()** # 关闭并行分词模式


**Tokenize：返回词语在原文的起止位置**

输入参数只接受 unicode

result = jieba.tokenize(u'永和服装饰品有限公司')

result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')


    import jieba
    import jieba.posseg as posseg
    
    sentence = '我喜欢上海金融国际中心'
    w1 = jieba.cut(sentence, cut_all=True)
    for item in w1:
        print(item)
    
    w2 = jieba.cut(sentence, cut_all=False)
    for item in w2:
        print(item)
    
    w3 = jieba.cut_for_search(sentence)
    for item in w3:
        print(item)
    
    w4 = posseg.cut(sentence)
    for word, flag in w4:
        print('%s----%s'%(word, flag))


