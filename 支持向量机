第一层、了解SVM
    支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
1.1、分类标准的起源：Logistic回归
    理解SVM，咱们必须先弄清楚一个概念：线性分类器。
    给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，
    用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面
    （hyper plane），这个超平面的方程可以表示为（ wT中的T代表转置）：

    可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。

    Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。


决策边界“越胖越好”，边界的泛化能力越强，分类能力越好     small margin ------large margin

soft margin 软间隔 :数据中的噪点

引入松弛因子 ：l - e


对待低维不可分的问题可以转换成高维可分的问题

低维空间的内积值映射到高位空间的内积值






核函数技巧

在以上例子中，我们找到了一种通过将空间巧妙地映射到更高维度来分类非线性数据的方法。然而事实证明，这种转换可能会带来很大的计算成本：可能会出现很多新的维度，每一个都可能带来复杂的计算。为数据集中的所有向量做这种操作会带来大量的工作，所以寻找一个更简单的方法非常重要。

还好，我们已经找到了诀窍：SVM 其实并不需要真正的向量，它可以用它们的数量积（点积）来进行分类。这意味着我们可以避免耗费计算资源的境地了。我们需要这样做：

想象一个我们需要的新空间：
z = x² + y²
找到新空间中点积的形式：
a · b = xa· xb + ya· yb + za· zb
a · b = xa· xb + ya· yb + (xa² + ya²) · (xb² + yb²)
让 SVM 处理新的点积结果——这就是核函数

这就是核函数的技巧，它可以减少大量的计算资源需求。通常，内核是线性的，所以我们得到了一个线性分类器。但如果使用非线性内核（如上例），我们可以在完全不改变数据的情况下得到一个非线性分类器：我们只需改变点积为我们想要的空间，SVM 就会对它忠实地进行分类。
注意，核函数技巧实际上并不是 SVM 的一部分。它可以与其他线性分类器共同使用，如逻辑回归等。支持向量机只负责找到决策边界。

支持向量机如何用于自然语言分类？

有了这个算法，我们就可以在多维空间中对向量进行分类了。如何将它引入文本分类任务呢？首先你要做的就是把文本的片断整合为一个数字向量，这样才能使用 SVM 进行区分。换句话说，什么属性需要被拿来用作 SVM 分类的特征呢？

最常见的答案是字频，就像在朴素贝叶斯中所做的一样。这意味着把文本看作是一个词袋，对于词袋中的每个单词都存在一个特征，特征值就是这个词出现的频率。

这样，问题就被简化为：这个单词出现了多少次，并把这个数字除以总字数。在句子「All monkeys are primates but not all primates are monkeys」中，单词 mokey 出现的频率是 2/10=0.2，而 but 的频率是 1/10=0.1。

对于计算要求更高的问题，还有更好的方案，我们也可以用 TF-IDF。
现在我们做到了，数据集中的每个单词都被几千（或几万）维的向量所代表，每个向量都表示这个单词在文本中出现的频率。太棒了！现在我们可以把数据输入 SVM 进行训练了。我们还可以使用预处理技术来进一步改善它的效果，如词干提取、停用词删除以及 n-gram。

选择核函数

现在我们有了特征向量，唯一要做的事就是选择模型适用的核函数了。每个任务都是不同的，核函数的选择有关于数据本身。在我们的例子中，数据呈同心圆排列，所以我们需要选择一个与之匹配的核函数。

既然需要如此考虑，那么什么是自然语言处理需要的核函数？我们需要费线性分类器吗？亦或是数据线性分离？事实证明，最好坚持使用线性内核，为什么？

回到我们的例子上，我们有两种特征。一些现实世界中 SVM 在其他领域里的应用或许会用到数十，甚至数百个特征值。同时自然语言处理分类用到了数千个特征值，在最坏的情况下，每个词都只在训练集中出现过一次。这会让问题稍有改变：非线性核心或许在其他情况下很好用，但特征值过多的情况下可能会造成非线性核心数据过拟合。因此，最好坚持使用旧的线性核心，这样才能在那些例子中获得很好的结果。

为我所用

现在需要做的就是训练了！我们需要采用标记文本集，使用词频将他们转换为向量，并填充算法，它会使用我们选择的核函数，然后生成模型。然后，当我们遇到一段未标记的文本想要分类时，我们就可以把它转化为向量输入模型中，最后获得文本类型的输出。

结语

以上就是支持向量机的基础。总结来说就是：
支持向量机能让你分类线性可分的数据；
如果线性不可分，你可以使用 kernel 技巧；
然而，对文本分类而言最好只用线性 kernel。
相比于神经网络这样更先进的算法，支持向量机有两大主要优势：更高的速度、用更少的样本（千以内）取得更好的表现。这使得该算法非常适合文本分类问题。



```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
%matplotlib inline

import seaborn as sns;sns.set()

#随机数据
from sklearn.datasets.samples_generator import make_blobs
X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.6)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

#训练一个基本的SVM
from sklearn.svm import SVC
model = SVC(kernel='linear')
model.fit(X, y)

# 绘图函数
def plot_svc_decision_function(model, ax=None, plot_support=True):
    if ax is None:
        ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    x = np.linspace(xlim[0], xlim[1], 30)
    y = np.linspace(ylim[0], ylim[1], 30)
    Y, X = np.meshgrid(y, x)
    xy = np.vstack([X.ravel(), Y.ravel()])
    P = model.decision_function(xy).reshape(X.shape)

    ax.counter(X, Y, P, colors='k',
              levels=[-1, 0, 1], alpha=0.5,
              linestyles=['--', '-', '--'])

    if plot_support:
        ax.scatter(model.support_vectors_[:, 0],
                  model.support_vectors_[:, 1],
                  s=300, linewidth=1, facecolors='none')
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model)

# 引入核函数
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
clf = SVC(kernel='linear').fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

#加入径向基函数
clf = SVC(kernel='rbf', C=1E6)
clf.fit(X, y)

X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)

```

人脸识别

```
from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)

fig, ax = plt.subplots(3, 5)
for i, axi in enumerate(ax.flat):
    axi.imshow(faces.images[i], cmap='bone')
    axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])

from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline

#降维
pca = PCA(n_components=150, whiten=True, random_state=42)
svc = SVC(kernel='rbf', class_weight='balanced')
model = make_pipeline(pca, svc)

from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target, random_state=40)

from sklearn.model_selection import GridSearchCV
param_grid = {'svc__C':[1, 5, 10], 'svc__gamma':[0.0001, 0.0005, 0.001]}
grid = GridSearchCV(model, param_grid)

%time grid.fit(Xtrain, ytrain)

model = grid.best_estimator_
yfit = model.predict(Xtest)
yfit.shape

fig, ax = plt.subplots(4, 6)
for i, axi in enumerate(ax.flat):
    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')
    axi.set(xticks=[], yticks=[])
    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1], color='black' if yfit[i] == ytest[i] else 'red')
fig.suptitle('Predicted names: Incorrect names in red', size=14)

```
