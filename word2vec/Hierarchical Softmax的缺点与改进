1. Hierarchical Softmax的缺点与改进
　　　　在讲基于Negative Sampling的word2vec模型前，我们先看看Hierarchical Softmax的的缺点。的确，使用霍夫曼树来代替传统的神经网络，可以提高模型训练的效率。但是如果我们的训练样本里的中心词ww是一个很生僻的词，那么就得在霍夫曼树中辛苦的向下走很久了。能不能不用搞这么复杂的一颗霍夫曼树，将模型变的更加简单呢？

　　　　Negative Sampling就是这么一种求解word2vec模型的方法，它摒弃了霍夫曼树，采用了Negative Sampling（负采样）的方法来求解，下面我们就来看看Negative Sampling的求解思路。

2. 基于Negative Sampling的模型概述
　　　　既然名字叫Negative Sampling（负采样），那么肯定使用了采样的方法。采样的方法有很多种，比如之前讲到的大名鼎鼎的MCMC。我们这里的Negative Sampling采样方法并没有MCMC那么复杂。

　　　　比如我们有一个训练样本，中心词是ww,它周围上下文共有2c2c个词，记为context(w)context(w)。由于这个中心词ww,的确和context(w)context(w)相关存在，因此它是一个真实的正例。通过Negative Sampling采样，我们得到neg个和ww不同的中心词wi,i=1,2,..negwi,i=1,2,..neg，这样context(w)context(w)和$$w_i$就组成了neg个并不真实存在的负例。利用这一个正例和neg个负例，我们进行二元逻辑回归，得到负采样对应每个词$w_i$对应的模型参数$\theta_{i}$，和每个词的词向量。

　　　　从上面的描述可以看出，Negative Sampling由于没有采用霍夫曼树，每次只是通过采样neg个不同的中心词做负例，就可以训练模型，因此整个过程要比Hierarchical Softmax简单。

　　　　不过有两个问题还需要弄明白：1）如果通过一个正例和neg个负例进行二元逻辑回归呢？ 2） 如何进行负采样呢？

　　　　我们在第三节讨论问题1，在第四节讨论问题2.

3. 基于Negative Sampling的模型梯度计算
　　　　Negative Sampling也是采用了二元逻辑回归来求解模型参数，通过负采样，我们得到了neg个负例(context(w),wi)i=1,2,..neg(context(w),wi)i=1,2,..neg。为了统一描述，我们将正例定义为w0w0。

　　　　在逻辑回归中，我们的正例应该期望满足：
P(context(w0),wi)=σ(xTwiθwi),yi=1,i=0
P(context(w0),wi)=σ(xwiTθwi),yi=1,i=0
　　　　我们的负例期望满足：
P(context(w0),wi)=1−σ(xTiθwi),yi=0,i=1,2,..neg
P(context(w0),wi)=1−σ(xiTθwi),yi=0,i=1,2,..neg
　　　　我们期望可以最大化下式：
∏i=0negP(context(w0),wi)=σ(xTw0θw0)∏i=1neg(1−σ(xTwiθwi))
∏i=0negP(context(w0),wi)=σ(xw0Tθw0)∏i=1neg(1−σ(xwiTθwi))
　　　　利用逻辑回归和上一节的知识，我们容易写出此时模型的似然函数为：
∏i=0negσ(xTwiθwi)yi(1−σ(xTwiθwi))1−yi
∏i=0negσ(xwiTθwi)yi(1−σ(xwiTθwi))1−yi
　　　　此时对应的对数似然函数为：
L=∑i=0negyilog(σ(xTwiθwi))+(1−yi)log(1−σ(xTwiθwi))
L=∑i=0negyilog(σ(xwiTθwi))+(1−yi)log(1−σ(xwiTθwi))
　　　　和Hierarchical Softmax类似，我们采用随机梯度上升法，仅仅每次只用一个样本更新梯度，来进行迭代更新得到我们需要的xwi,θwi,i=0,1,..negxwi,θwi,i=0,1,..neg, 这里我们需要求出xwi,θwi,i=0,1,..negxwi,θwi,i=0,1,..neg的梯度。

　　　　首先我们计算θwiθwi的梯度：
∂L∂θwi=yi(1−σ(xTwiθwi))xwi−(1−yi)σ(xTwiθwi)xwi=(yi−σ(xTwiθwi))xwi(1)(2)
(1)∂L∂θwi=yi(1−σ(xwiTθwi))xwi−(1−yi)σ(xwiTθwi)xwi(2)=(yi−σ(xwiTθwi))xwi
　　　　同样的方法，我们可以求出xwixwi的梯度如下：
∂L∂xwi=(yi−σ(xTwiθwi))θwi
∂L∂xwi=(yi−σ(xwiTθwi))θwi
　　　　有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的xwi,θwi,i=0,1,..negxwi,θwi,i=0,1,..neg。

4. Negative Sampling负采样方法
　　　　现在我们来看看如何进行负采样，得到neg个负例。word2vec采样的方法并不复杂，如果词汇表的大小为VV,那么我们就将一段长度为1的线段分成VV份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。每个词ww的线段长度由下式决定：
len(w)=count(w)∑u∈vocabcount(u)
len(w)=count(w)∑u∈vocabcount(u)
　　　　在word2vec中，分子和分母都取了3/4次幂如下：
len(w)=count(w)3/4/∑u∈vocabcount(u)3/4
len(w)=count(w)3/4∑u∈vocabcount(u)3/4
　　　　在采样前，我们将这段长度为1的线段划分成MM等份，这里M>>VM>>V，这样可以保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从MM个位置中采样出negneg个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。



　　　　在word2vec中，MM取值默认为10^8。


5.  基于Negative Sampling的CBOW模型
　　　　有了上面Negative Sampling负采样的方法和逻辑回归求解模型参数的方法，我们就可以总结出基于Negative Sampling的CBOW模型算法流程了。梯度迭代过程使用了随机梯度上升法：

　　　　输入：基于CBOW的语料训练样本，词向量的维度大小MM，CBOW的上下文大小2c2c,步长ηη, 负采样的个数neg

　　　　输出：词汇表每个词对应的模型参数θθ，所有的词向量xwxw
　　　　1. 随机初始化所有的模型参数θθ，所有的词向量ww
　　　　2. 对于每个训练样本(context(w0),w0)(context(w0),w0),负采样出neg个负例中心词wi,i=1,2,...negwi,i=1,2,...neg
　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(context(w0),w0,w1,...wneg)(context(w0),w0,w1,...wneg)做如下处理：

　　　　　　a)  e=0， 计算xw0=12c∑i=12cxixw0=12c∑i=12cxi
　　　　　　b)  for i= 0 to neg, 计算：
f=σ(xTwiθwi)
f=σ(xwiTθwi)
g=(yi−f)η
g=(yi−f)η
e=e+gθwi
e=e+gθwi
θwi=θwi+gxwi
θwi=θwi+gxwi
　　　           c) 对于context(w)context(w)中的每一个词向量xkxk(共2c个)进行更新：
xk=xk+e
xk=xk+e
　

　　　　　　d) 如果梯度收敛，则结束梯度迭代，否则回到步骤3继续迭代。

6.  基于Negative Sampling的Skip-Gram模型
　　　　有了上一节CBOW的基础和上一篇基于Hierarchical Softmax的Skip-Gram模型基础，我们也可以总结出基于Negative Sampling的Skip-Gram模型算法流程了。梯度迭代过程使用了随机梯度上升法：

　　　　输入：基于Skip-Gram的语料训练样本，词向量的维度大小MM，Skip-Gram的上下文大小2c2c,步长ηη， , 负采样的个数neg。

　　　　输出：词汇表每个词对应的模型参数θθ，所有的词向量xwxw
　　　　1. 随机初始化所有的模型参数θθ，所有的词向量ww
　　　　2. 对于每个训练样本(context(w0),w0)(context(w0),w0),负采样出neg个负例中心词wi,i=1,2,...negwi,i=1,2,...neg
　　　　3. 进行梯度上升迭代过程，对于训练集中的每一个样本(context(w0),w0,w1,...wneg)(context(w0),w0,w1,...wneg)做如下处理：

　　　　　　a)  for i =1 to 2c:

　　　　　　　　i)  e=0

　　　　　　　　ii)  for j= 0 to neg, 计算：
f=σ(xTw0iθwj)
f=σ(xw0iTθwj)
g=(yj−f)η
g=(yj−f)η
e=e+gθwj
e=e+gθwj
θwj=θwj+gxw0i
θwj=θwj+gxw0i
　　　　　　　　iii)  对于context(w)context(w)中的每一个词向量xkxk(共2c个)进行更新：
xk=xk+e
xk=xk+e
　　　　　　b)如果梯度收敛，则结束梯度迭代，算法结束，否则回到步骤a继续迭代。

7.  Negative Sampling的模型源码和算法的对应　　
　　　　这里给出上面算法和word2vec源码中的变量对应关系。

　　　　在源代码中，基于Negative Sampling的CBOW模型算法在464-494行，基于Hierarchical Softmax的Skip-Gram的模型算法在520-542行。大家可以对着源代码再深入研究下算法。

　　　　在源代码中，neule对应我们上面的ee, syn0对应我们的xwxw, syn1neg对应我们的θwiθwi, layer1_size对应词向量的维度，window对应我们的cc。negative对应我们的neg, table_size对应我们负采样中的划分数MM。

　　　　另外，vocab[word].code[d]指的是，当前单词word的，第d个编码，编码不含Root结点。vocab[word].point[d]指的是，当前单词word，第d个编码下，前置的结点。这些和基于Hierarchical Softmax的是一样的。

　　　　以上就是基于Negative Sampling的word2vec模型，希望可以帮到大家，后面会讲解用gensim的python版word2vec来使用word2vec解决实际问题。



```
// NEGATIVE SAMPLING
        if (negative > 0) for (d = 0; d < negative + 1; d++) {
          if (d == 0) {
            target = word;
            label = 1;
          } else {
            next_random = next_random * (unsigned long long)25214903917 + 11;
            target = table[(next_random >> 16) % table_size];
            if (target == 0) target = next_random % (vocab_size - 1) + 1;
            if (target == word) continue;
            label = 0;
          }
          l2 = target * layer1_size;
          f = 0;
          for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1neg[c + l2];
          if (f > MAX_EXP) g = (label - 1) * alpha;
          else if (f < -MAX_EXP) g = (label - 0) * alpha;
          else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
          for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * neu1[c];
        }
        // hidden -> in
        for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
          c = sentence_position - window + a;
          if (c < 0) continue;
          if (c >= sentence_length) continue;
          last_word = sen[c];
          if (last_word == -1) continue;
          for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];
        }
      }
```