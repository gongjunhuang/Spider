贝叶斯

优点：
对待预测样本进行预测，过程简单速度快(想想邮件分类的问题，预测就是分词后进行概率乘积，在log域直接做加法更快)。
对于多分类问题也同样很有效，复杂度也不会有大程度上升。
在分布独立这个假设成立的情况下，贝叶斯分类器效果奇好，会略胜于逻辑回归，同时我们需要的样本量也更少一点。
对于类别类的输入特征变量，效果非常好。对于数值型变量特征，我们是默认它符合正态分布的。

缺点：
对于测试集中的一个类别变量特征，如果在训练集里没见过，直接算的话概率就是0了，预测功能就失效了。当然，我们前面的文章提过我们有一种技术叫做**『平滑』操作**，可以缓解这个问题，最常见的平滑技术是拉普拉斯估测。
那个…咳咳，朴素贝叶斯算出的概率结果，比较大小还凑合，实际物理含义…恩，别太当真。
朴素贝叶斯有分布独立的假设前提，而现实生活中这些predictor很难是完全独立的。

最常见应用场景

文本分类/垃圾文本过滤/情感判别：这大概会朴素贝叶斯应用做多的地方了，即使在现在这种分类器层出不穷的年代，
在文本分类场景中，朴素贝叶斯依旧坚挺地占据着一席之地。原因嘛，大家知道的，因为多分类很简单，同时在文本数据中，
分布独立这个假设基本是成立的。而垃圾文本过滤(比如垃圾邮件识别)和情感分析(微博上的褒贬情绪)用朴素贝叶斯也通常能取得很好的效果。

多分类实时预测：这个是不是不能叫做场景？对于文本相关的多分类实时预测，它因为上面提到的优点，被广泛应用，简单又高效。

推荐系统：是的，你没听错，是用在推荐系统里！！朴素贝叶斯和协同过滤(Collaborative Filtering)是一对好搭档，协同过滤是强相关性，但是泛化能力略弱，朴素贝叶斯和协同过滤一起，能增强推荐的覆盖度和效果。

正向概率：假设袋子里面有N个白球，M个黑球，取黑球的概率多大

逆向概率：事先不知道袋子里黑白球的比例，直接摸一个或者多个球，观察取出来之后球的颜色，
可以就此堆袋子里面黑白球的比例作出什么样的推测

编辑距离：
两个词之间的编辑距离定义为使用了几次插入、删除、交换、替换的操作把一个词变成另一个词

```
import re, collections

def words(text):
    return re.findall('[a-z]+', text.lower())

def train(feature):
    model = collections.defaultdict(lambda: 1)
    for f in feature:
        model[f] += 1
    return model

NWORDS = train(words(open('big.txt').read()))
alphabet = 'abcdefghijklmnopqrstuvwxyz'

def edits1(words):
    n = len(word)
    return set([word[0:i] + word[i+1:] for i in range(n)] +                             #deletion
              [word[0:i] + word[i+1:] + word[i] + word[i+2:] for i in range(n-1)] +     #transposition
              [word[0:i] + c + word[i+1:] for i in range(n) for c in alphabet] +        #alteration
              [word[0:i] + c + word[i:] for i in range(n) for c in alphabet])           #insertion

def edits2(words):
    return set(e2 for e1 in edits1(words) for e2 in edits1(e1))

def known(words):
    return set(w for w in words if w in NWORDS)

def correct(word):
    candidates = known([word]) or known(edits1(word)) or known(edits2(word)) or [word]
    return max(candidates, key=lambda w: NWORDS[w])

```

建模：

高斯分布型：用于classification问题，假定属性/特征是服从正态分布的。

多项式型：用于离散值模型里。比如文本分类问题里面我们提到过，我们不光看词语是否在文本中出现，也得看出现的次数。如果总词数为n，出现词数为m的话，说起来有点像掷骰子n次出现m次这个词的场景。

伯努利型：这种情况下，就如之前博文里提到的bag of words处理方式一样，最后得到的特征只有0(没出现)和1（出现过）
