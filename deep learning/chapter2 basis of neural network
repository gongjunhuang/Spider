神经网络基础
实现一个神经网络时，如果需要遍历整个训练集，并不需要直接使用 for 循环。
神经网络的计算过程中，通常有一个正向过程（forward pass）或者叫正向传播步骤（forward propagation step），接着会有一个反向过程（backward pass）或者叫反向传播步骤（backward propagation step）。
Logistic 回归

Logistic 回归是一个用于二分分类的算法。

Logistic 回归中使用的参数如下：

输入的特征向量：x∈Rnx，其中 nx是特征数量；
用于训练的标签：y∈0,1
权重：w∈Rnx
偏置： b∈R
输出：y^=σ(wTx+b)
Sigmoid 函数：
s=σ(wTx+b)=σ(z)=11+e−z
为将 wTx+b 约束在 [0, 1] 间，引入 Sigmoid 函数。从下图可看出，Sigmoid 函数的值域为 [0, 1]。

损失函数
损失函数（loss function）用于衡量预测结果与真实值之间的误差。

最简单的损失函数定义方式为平方差损失：
L(y^,y)=12(y^−y)2
但 Logistic 回归中我们并不倾向于使用这样的损失函数，因为之后讨论的优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。

一般使用
L(y^,y)=−(ylogy^)−(1−y)log(1−y^)
损失函数是在单个训练样本中定义的，它衡量了在单个训练样本上的表现。而代价函数（cost function，或者称作成本函数）衡量的是在全体训练样本上的表现，即衡量参数 w 和 b 的效果。

J(w,b)=1m∑i=1mL(y^(i),y(i))


梯度下降法（Gradient Descent）
函数的梯度（gradient）指出了函数的最陡增长方向。即是说，按梯度的方向走，函数增长得就越快。那么按梯度的负方向走，函数值自然就降低得最快了。

模型的训练目标即是寻找合适的 w 与 b 以最小化代价函数值。

可以看到，成本函数 J 是一个凸函数，与非凸函数的区别在于其不含有多个局部最低点；选择这样的代价函数就保证了无论我们初始化模型参数如何，都能够寻找到合适的最优解。

参数 w 的更新公式为：

w:=w−αdJ(w,b)dw
其中 α 表示学习速率，即每次更新的 w 的步伐长度。

当 w 大于最优解 w′ 时，导数大于 0，那么 w 就会向更小的方向更新。反之当 w 小于最优解 w′ 时，导数小于 0，那么 w 就会向更大的方向更新。迭代直到收敛。

在成本函数 J(w, b) 中还存在参数 b，因此也有：

b:=b−αdJ(w,b)db

计算图（Computation Graph）
神经网络中的计算即是由多个计算网络输出的前向传播与计算梯度的后向传播构成。所谓的反向传播（Back Propagation）即是当我们需要计算最终值相对于某个特征变量的导数时，我们需要利用计算图中上一步的结点定义。



向量化（Vectorization）
在 Logistic 回归中，需要计算
z=wTx+b
如果是非向量化的循环方式操作，代码可能如下：

z = 0;
for i in range(n_x):
    z += w[i] * x[i]
z += b
而如果是向量化的操作，代码则会简洁很多，并带来近百倍的性能提升（并行指令）：

z = np.dot(w, x) + b
不用显式 for 循环，实现 Logistic 回归的梯度下降一次迭代（对应之前蓝色代码的 for 循环部分。这里公式和 NumPy 的代码混杂，注意分辨）：

Z=wTX+b=np.dot(w.T,x)+b
A=σ(Z)
dZ=A−Y
dw=1mXdZT
db=1mnp.sum(dZ)
w:=w−σdw
b:=b−σdb
正向和反向传播尽管如此，多次迭代的梯度下降依然需要 for 循环。


广播（broadcasting）
Numpy 的 Universal functions 中要求输入的数组 shape 是一致的。当数组的 shape 不相等的时候，则会使用广播机制，调整数组使得 shape 一样，满足规则，则可以运算，否则就出错。

四条规则：

让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分都通过在前面加 1 补齐；
输出数组的 shape 是输入数组 shape 的各个轴上的最大值；
如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错；
当输入数组的某个轴的长度为 1 时，沿着此轴运算时都用此轴上的第一组值。




NumPy 使用技巧
转置对秩为 1 的数组无效。因此，应该避免使用秩为 1 的数组，用 n * 1 的矩阵代替。例如，用np.random.randn(5,1)代替np.random.randn(5)。

如果得到了一个秩为 1 的数组，可以使用reshape进行转换。